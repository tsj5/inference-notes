% !TEX TS-program = latexmk

% partial fixes for ``cannot redefine operator''
\let\negmedspace\undefined
\let\negthickspace\undefined
\RequirePackage{amsmath} 

\documentclass[notitlepage,openany,11pt]{report}
%\documentclass[11pt,oneside]{amsart}
\usepackage[pdftex,letterpaper,
total={5.75in, 9in},left=0.75in,marginparwidth=1.25in]{geometry}
\usepackage[pdftex]{graphicx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% fonts and encoding
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel} % hyphenation etc.
\usepackage{csquotes}%
\usepackage{newtxtext,newtxmath}
% \usepackage{times} % times font for text, not equations
\usepackage{microtype} % kerning

% math
\let\Bbbk\relax
\let\openbox\relax
%\usepackage{amsmath} % math
\usepackage{amsfonts,amssymb,amsthm}
\usepackage{bm}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}

% other packages
\usepackage{adjmulticol} % 2-col for biblio
\usepackage{indentfirst}
\usepackage{enumitem}
\setlist{noitemsep}
\usepackage{tabularx}
\usepackage{marginnote}
\renewcommand*{\marginfont}{\footnotesize}

% Draw box around projects
\usepackage{framed}
\theoremstyle{plain}% default
\newtheorem{notethm}{Remark}
\newenvironment{notebox}
    {\noindent\colorlet{shadecolor}{cyan!15}\begin{shaded}\begin{notethm}}
    {\end{notethm}\end{shaded}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% chapter/section headings

\usepackage{sectsty} % reduce font size for chapter headers
\chapterfont{\LARGE}
\chapternumberfont{\LARGE}
\chaptertitlefont{\LARGE}
\sectionfont{\Large}
\subsectionfont{\large}
\subsubsectionfont{\normalsize}
\usepackage[titles]{tocloft}
\renewcommand{\cftchapfont}{\normalfont\bfseries}% titles in bold
\renewcommand{\cftsecfont}{\normalfont\bfseries}% titles in bold
\renewcommand{\cftsecpagefont}{\normalfont\bfseries}% page numbers in bold
% \renewcommand{\cftdotsep}{1}

% Number subsubsections for references, but don't put them in the TOC
\setcounter{secnumdepth}{3} 
\setcounter{tocdepth}{2}
 
% Number subsubsections for references, but don't put them in the TOC
% don't number chapters in order to keep TOC, text refs manageable
\newcommand{\nonumberchapter}[1]{%
    %\setcounter{section}{0}
    \chapter*{#1}
    \vspace{-20pt}
    \addcontentsline{toc}{chapter}{#1}
}
\renewcommand{\thesection}{\arabic{section}}
\newcommand{\nonumbersection}[1]{%
    \setcounter{subsection}{0}
    \section*{#1}
    \addcontentsline{toc}{section}{#1}
}
\numberwithin{equation}{section}

% put TOC on title page, 
% https://tex.stackexchange.com/questions/45861/toc-on-the-title-page-in-a-report
\makeatletter
\newcommand*{\toccontents}{\@starttoc{toc}}
\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Biblio

\usepackage[square,numbers,sort&compress]{natbib} % ,merge,elide

\usepackage{xcolor}
\definecolor{darkred}{rgb}{0.45,0.,0.}
\definecolor{darkgreen}{rgb}{0.,0.45,0.}
\definecolor{darkblue}{rgb}{0.,0.,0.5}

\usepackage[hyphens,obeyspaces]{url}
\usepackage[%
    breaklinks,colorlinks=true,
    linkcolor=darkred,citecolor=darkgreen,urlcolor=darkblue,
    backref=page
]{hyperref}
% \hypersetup{backref=page} % needs to be specified post-load?
\usepackage{hypernat} % after hyperref and natbib
%\PassOptionsToPackage{hyphens,obeyspaces}{url}
\usepackage{doi} % needs to be after hyperref and url?


% Eliminate ``References'' header from \bibliography since
% we put that in manually, because we want it in TOC
\renewcommand{\bibsection}{}

% Don't complain about exported language=en tags in biblio
% https://tex.stackexchange.com/a/199299
\makeatletter
\let\ORIbbl@fixname\bbl@fixname
\def\bbl@fixname#1{%
    \@ifundefined{languagealias@\expandafter\string#1}
        {\ORIbbl@fixname#1}
        {\edef\languagename{\@nameuse{languagealias@#1}}}%
}
\newcommand{\definelanguagealias}[2]{%
    \@namedef{languagealias@#1}{#2}%
}
\makeatother
\definelanguagealias{en}{english}
\definelanguagealias{en-US}{english}

% patch backref option to link to line of citation
% must be applied after hyperref loaded w/appropriate backref option
% ``merge'' option for natbib still broken, though.
% From https://tex.stackexchange.com/a/67852, see there for comments

% If there are multiple cites on the same page, should we show only the
% first one or should we show them all?
\newif\ifbackrefshowonlyfirst
\backrefshowonlyfirsttrue % or false

\makeatletter
\let\BR@direct@old@hyper@natlinkstart\hyper@natlinkstart
\renewcommand*{\hyper@natlinkstart}{\phantomsection\BR@direct@old@hyper@natlinkstart}%
\let\BR@direct@oldBR@citex\BR@citex
\renewcommand*{\BR@citex}{\phantomsection\BR@direct@oldBR@citex}%

\long\def\hyper@page@BR@direct@ref#1#2#3{\hyperlink{#3}{#1}}

\ifx\backrefxxx\hyper@page@backref
    \let\backrefxxx\hyper@page@BR@direct@ref
    \ifbackrefshowonlyfirst
        %\let\backrefxxxdupe\hyper@page@backref%
        \newcommand*{\backrefxxxdupe}[3]{#1}%
    \fi
\else
    \ifbackrefshowonlyfirst
        \newcommand*{\backrefxxxdupe}[3]{#2}%
    \fi
\fi
\RequirePackage{etoolbox}
\patchcmd{\Hy@backout}{Doc-Start}{\@currentHref}{}{\errmessage{I can't patch backref}}
\makeatother

% Add text to bibitem explaining what backrefs are doing
% from https://tex.stackexchange.com/a/397886
\renewcommand\backreftwosep{, }
\renewcommand\backrefsep{, }
\renewcommand*{\backrefalt}[4]{%
    \ifcase #1%
        \or Cited on~p.~#2.%
        \else Cited on~pp.~#2.%
    \fi%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\newcommand {\be}{\begin{equation*}}
\newcommand {\ee} {\end{equation*}}
\newcommand{\boldref}[1]{\textbf{\ref{#1}}}
\newcommand{\boldnameref}[1]{\textbf{\nameref{#1}}}

\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mtilde}[1]{\widetilde{#1}}
\newcommand{\mhat}[1]{\widehat{#1}}
\newcommand{\mol}[1]{\overline{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Background on Bayesian machine learning, inference and control}
\author{Tom Jackson}
%\email[]{Your e-mail address}
%\affiliation{}

\hypersetup{pageanchor=false} % fix hyperref error: https://tex.stackexchange.com/a/331766

%\begin{titlepage}
\newgeometry{total={5.75in, 9in}} % https://stackoverflow.com/questions/1670463
\maketitle

\begin{abstract}
This is a (disorganized) summary writeup on with pointers to literature I've compiled for my own use.\end{abstract}

\hypersetup{pageanchor=true}
\pagenumbering{arabic}

%\addtocontents{toc}{\vspace{-10pt}}
%\tableofcontents
\toccontents

\restoregeometry

%\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Probability}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Gaussians}

\subsection{Gaussian integrals}
\be
\int \! dx \, e^{-x^{2}} = \sqrt{\pi}.
\ee
Gaussian integral via completing the square:
\be
\int \! d^{d}\mbf{x} \, \exp \left[ - \tfrac{1}{2} \mbf{x}^{T} \mbf{A} \mbf{x} + \mbf{Jx} \right] = \sqrt{\frac{(2 \pi)^{d}}{\det \mbf{A}}} \exp \left[ - \tfrac{1}{2} \mbf{J}^{T} \mbf{A}^{-1} \mbf{J} \right].
\ee

\subsection{Normal distributions}

Ref: \href{https://en.wikipedia.org/wiki/Multivariate_normal_distribution}{wiki}.

Denote $X \sim \mcal{N}(\mbf{\mu}, \mbf{\Sigma})$ for a normally distributed random variable $X$:
\be
p(X= \mbf{x}) = \frac{1}{\sqrt{(2 \pi)^{d} \det \mbf{\Sigma}}} \exp \left[ - \tfrac{1}{2} (\mbf{x}- \mbf{\mu})^{T} \mbf{\Sigma}^{-1} (\mbf{x} - \mbf{\mu}) \right],
\ee
with $\mbf{\Sigma}$ positive definite. 

If $X \sim \mcal{N}(\mbf{\mu}, \mbf{\Sigma})$, an affine transformation $Y = \mbf{H}X + \mbf{c}$ is distributed as $Y \sim \mcal{N}(\mbf{H\mu} + \mbf{c}, \mbf{H\Sigma H}^{T})$. Implies marginal distribution just drops relevant entries from $\mbf{\mu}, \mbf{\Sigma}$. 

Note that if $X_1, X_2$ are normal, their joint distribution need not be, even if they're uncorrelated [$\cov(X_1, X_2) = 0$]. 

Conditioning a Gaussian: Take $X_1, X_2$ joint normal, with $\mbf{\Sigma}$ having a $2 \times 2$ block structure. Conditional distribution $p(X_1 | X_2 = \mbf{x}_2) = p(X_1, X_2)/ p(X_2)$ can be read off using the \href{https://en.wikipedia.org/wiki/Schur_complement}{Schur identity}
\be
\begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{bmatrix}^{-1} = \begin{bmatrix} S & -S \Sigma_{12} \Sigma_{22}^{-1}  \\ - \Sigma_{22}^{-1} \Sigma_{21} S & \Sigma_{22}^{-1} + \Sigma_{22}^{-1} \Sigma_{21} S \Sigma_{12} \Sigma_{22}^{-1} \end{bmatrix}
\ee
with
\be
S \equiv \left( \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} \right)^{-1}.
\ee
Marginal $1/p(X_2)$ only cancels constant term. Reading off new mean and covariance from linear and quadratic $X_1$ dependence, we get
\be
X_1 | X_2 = \mbf{x}_2 \sim \mcal{N} \left( \mbf{\mu}_{1} + \Sigma_{12} \Sigma_{22}^{-1} (\mbf{x}_{2} - \mbf{\mu}_2), S^{-1} \right).
\ee
Note that conditioning on $X_2$ shifts the mean for $X_1$, which wouldn't have happened if we just took the marginal $p(X_1)$.



\section{Information theory}

\subsection{Definitions}

\subsubsection{Entropy}
\be
H(X) = \int\! dx \, p(x) \log p(x) \geq 0.
\ee

\subsubsection{Kullback-Leibler divergence}
\be
D_{KL}[P | Q] = \int\! dx \, p(x) \log \frac{p(x)}{q(x)} \geq 0,
\ee
with equality only for $P=Q$. Convex.

Also referred to as relative entropy. Asymmetric between $P$ and $Q$, to reflect Bayesian inference: $D_{KL}[P | Q]$ is information gain from revising prior $Q$ to posterior $P$.

\subsubsection{Evidence lower bound}
Fact that $D_{KL}$ is always positive yields a relevant bound for variational inference. Let $X$ be a latent variable and $Z$ an observation. Then
\be
D_{KL}[Q(X) | P(X|Z)] = \left\langle \log \frac{Q(X)}{P(X|Z)} \right\rangle_{X \sim Q} = \left\langle \log P(Z) - \log \frac{P(X,Z)}{Q(X)} \right\rangle_{X \sim Q};
\ee
$\log P(Z)$ is independent of $X$, so it can be taken outside the expectation, and the LHS is $\geq 0$, so 
\be
\log P(Z) \geq \left\langle \log \frac{P(X,Z)}{Q(X)} \right\rangle_{X \sim Q}.
\ee
The RHS is the EBLO objective. Finding the arg-max of this objective over all distributions $Q(X)$ drives $Q(X)$ to the posterior $P(X|Z)$, so this can be taken as the starting point for \href{https://en.wikipedia.org/wiki/Variational_Bayesian_methods}{variational Bayesian inference}. 


\subsubsection{Fisher information}

KL divergence is asymmetric, hence not a metric (some parallels to squared distance). ``Infinitesimal form,'' in the following sense, is a valid metric. Assume $P$, $Q$ depend on parameters $\theta$, and let $P$ be a small perturbation of $Q$: $P_{\theta} = Q_{\theta_{0} + \delta \theta}$. Then the hessian
\be
g_{ij}(\theta_{0}) = \frac{\partial^{2}}{\partial \theta_{i} \, \partial \theta_{j}} D_{KL}\left[ P_{\theta} | Q_{\theta_{0}} \right]
\ee
is a positive semidefinite Riemannian metric on the space of parameters $\theta$.

\subsubsection{Mutual information}
\be
I(X;Y) = D_{KL}[p(x,y) | p(x)p(y)] = \int\! dx \, dy \, p(x,y) \log \frac{p(x,y)}{p(x)p(y)}  \geq 0.
\ee


[...]

\subsection{Recent work on mutual information estimators.}

\subsubsection{Mutual Information Neural Estimation}
\cite{BelghaziEtAl:18}; no first-party code but third-party implementations; e.g. \href{https://github.com/gtegner/mine-pytorch}{[1]}, \href{https://github.com/MasanoriYamada/Mine_pytorch}{[2]}.

Application of neural network methods to estimate mutual information from a set of samples. Wide range of nonparametric estimators in prior literature (reviewed in \cite{Paninski:03}), but argue they don't scale to high data dimensionality (also gradients can be ill-defined or expensive).

General principle used often in this and following sections: derive a functional bound for the true MI. Use this as a loss function and implement the function as a neural network, ``trained'' by standard parameter optimization. The best value of the loss function is then the estimator's estimate.

Here the bound used is
\be
D_{KL}[P | Q] = \sup_{f} \left[ \langle f \rangle_{P} - \log \langle e^f \rangle_{Q} \right],
\ee
for arbitrary scalar function $f$ having the same domain as $P,Q$; this is used to get a variational lower bound for MI. Naive estimation of the gradient is biased, so an exponential weighted moving average is used [???]. For MI, minibatch sampling is applied; $P = p(x,y)$ is the empirical distribution of the batch, and $Q = p(x) \otimes p(y)$ is generated by sampling from the marginals, or by permuting one member of the $(x,y)$ pairs. 

\subsubsection{Related work and follow-ups}

Related work on MINE-style estimation:
\begin{itemize}
\item \cite{NguyenEtAl:10}: Earlier estimator (NWJ) using similar but looser bound.
\item \cite{LiaoEtAl:20} \href{https://openreview.net/forum?id=3LujMJM9EMp}{comments}; \href{https://github.com/RayRuizhiLiao/demi_mi_estimator}{code}. Learn a network that discriminates whether samples came from the joint or product-marginal distribution. Doesn't outperform state of the art, and reviewer points out it's a specific case of ideas in NWJ.
\item \cite{OordEtAl:19}: Another estimator (InfoNCE), based on contrast predictive coding.
\item \cite{LinEtAl:19}; \href{https://openreview.net/forum?id=SklOypVKvS}{comments}. Propose modification of MINE training to make it more data-efficient; unclear as to how --- purely by separating data into train/test sets?
\item \cite{ChanEtAl:19}; \href{https://github.com/ccha23/MI-NEE}{code}. Also addresses efficiency of MINE; claim improvement by estimating entropies as an intermediate step.
\item \cite{WenEtAl:20}; \href{https://openreview.net/forum?id=ByxaUgrFvH}{comments} propose a nontrivial estimator for the gradient of MI directly that may be more relevant when it's used in a loss function.
\end{itemize}

More general discussion:
\begin{itemize}
\item \cite{McAllesterStratos:20}; \href{https://openreview.net/forum?id=BkedwoC5t7}{comments}. Argue that there are fundamental limitations to the entire variational lower bound program. 
\item \cite{PooleOzair:19}: Compare variational methods in existing literature. Nothing yields good estimates for practical batch sizes, perhaps due to logic in previous ref. Propose new estimator $I_{\alpha}$ interpolating between InfoNCE and NWJ.
\item \cite{SongErmon:20}; \href{https://openreview.net/forum?id=B1x62TNtDS}{comments}. Elaborate on bias/variance tradeoff; variance in estimates from MINE et al. can blow up due to instability. \href{https://github.com/ermongroup/smile-mi-estimator}{Code} for an improved estimator (SMILE).
\item \cite{ChoiLee:20}; \href{https://openreview.net/forum?id=Lvb2BKqL49a}{comments} (code in supplementary material). Propose to  regularize variance blow-ups in MINE by adding a term $- d( \log \langle e^f \rangle_{Q}, C)$ where $d$ is a 1d distance function and $C$ is an arbitrary constant; use $\lambda (x-C)^{2}$.
\end{itemize}

\begin{notebox}
Would appear that best current estimator is either \cite{ChanEtAl:19} or \cite{ChoiLee:20}; former suffers from not providing comparison with other methods.
\end{notebox}

\subsubsection{Contrastive Log-ratio Upper Bound}
\cite{ChengEtAl:20}; \href{https://github.com/Linear95/CLUB}{code}.

Prior work focusing on \emph{upper} bounds on MI (needed for MI minimization objectives) required knowledge of $p(y|x)$, e.g. the approximation to the marginal in InfoBot (\boldref{sssec-infobot}). If $p(y|x)$ is known, use
\begin{align*}
I(X;Y) &\leq \langle \log p(y|x) \rangle_{p(x,y)} - \langle \log p(y|x) \rangle_{p(x) \otimes p(y)}, \\
{} &= \frac{1}{N^2} \sum_{i,j}^{N} \log(y_i|x_i) - \log(y_j|x_i);
\end{align*}
If $p(y|x)$ isn't known, represent it as a NN. The double sum can be improved to a single loop over the data by using a random sample of the $\{ y_{j} \}$, similar to MINE. 





\section{Probabilistic graphical models}

\subsection{Markov chains.} 
Generic model of a memoryless process. Take a discretized state space $S \in \mcal{S}$ in discrete time and describe stochastic time evolution through matrix of conditional transition probabilities
\be
T_{ij} = \text{Pr}(S_{t+1} = s_{i} | S_{t} = s_{j}).
\ee
($S_{t+1}, S_{t}...$ are discrete random variables, while $s_{i}, s_{j}$ label state space elements.)
\marginnote{Is there a model of chaotic dynamics that's discrete in both space and time? Possible to define attractor for cellular automata?}

$T$ is a ``right stochastic matrix'': all entries are non-negative and rows sum to 1. (continuous version: kernel; volume preserving diffeos). Stationary distributions $\pi$ are left eigenvectors: $\pi T = \pi$. \emph{If} $T$ is irreducible (one connected component) and aperiodic (aka \emph{ergodic}), stationary distribution is unique Perron-Frobenius eignevector of eigenvalue $1$.

Detailed balance condition (aka ``reversibility'') is that there exists a $\pi$ such that:
\be
\pi_{i} T_{ij} = \pi_{j} T_{ji}.
\ee
If $\pi$ exists, it's a steady-state distribution. For any $T$, $\pi$ and matrix norm, can find a reversible $T^{\ast}$ closest to $T$ preserving $\pi$ through quadratic convex optimization. Doubly-stochastic matrices have a neat combinatorical description (Birkhoff polytope).



\subsection{Hidden Markov models.} 
\label{sec-hidden-markov-models}

Add unobserved, discrete latent variables (observed variables may be discrete or continuous.)

\subsubsection{Graphical models.} Depiction of the factorization of a general distribution. Graph nodes are individual random variables (or components), with an arrow $v \to v'$ if the distribution factorizes as $p(S_{v'} | \cdots, S_{v}, \cdots)$. Markov chain is just a linear chain, while hidden Markov model has a comb structure: latent variables $\{Z_{t} \}$ by themselves are a Markov chain, while observable $S_{t}$ depends on latent $z_{t}$ at that time only. When limited to observables, $S_{t}$ is not conditionally independent of any other $S$ in its past.

At fixed $t$, can view as a mixture of distribution model (with mixture components labeled by values of $z_{t-1}$. Can view as an example of independent component analysis (with the hidden variables labeling the components.) 



[...]


\subsection{Markov Decision Processes} 

\subsubsection{Terminology} Pick up from discussion of Markov chains; remain in discrete setting. Augment state with choice of action $A_{t}$ taken at time $t$, and reward $R_{t}$. Transition probability now takes the form $T(s_{+}, r | s, a)$, where we abbreviate $s_{+} = s_{t+1}$; note that this form allows for stochasticity in actions and rewards, since deterministic case is a special case of this. Can show that optimal policy for an MDP where everything is known is always deterministic, but stochastic policies can be optimal for POMDPs. 

Also retain critical feature that $s_{+}$ ``depends only on $s$,'' \marginnote{Martingale? Is this what's meant by a ``separator''?} which is reflected in ansatz for stochastic policy function $\pi(a|s)$ --- in particular, $A$ isn't merely enlarging state space. Stated problem is to select $\pi$ to maximize discounted total reward $\sum_{t} \gamma^{t} R(s_{t}, a_{t})$. At this point we only introduce discounting factor $0 < \gamma < 1$ for convergence's sake.

Note this is a bit orthogonal to traditional ML: ``semi-supervised'' learning in that we learn reward, but it's up to the agent to determine how to relate that to policy. Also learning problems in general either omit feedback (offline) or tend to leave it implicit in the online case.


\section{Monte Carlo} 

\subsection{Intro} Ref: \cite{MacKay:03} ch. 29.

Recall that Monte Carlo integration, at its simplest, estimates a high-dimensional integral by randomly sampling positions in the domain of integration \marginnote{Or quasi-randomly sampling; see low-discrepancy sequences.} and taking the average of values of the integrand evaluated at those points. This is wasteful if we draw lots of samples from areas where the integrand is small.

A special case of integration is computing expectation values of arbitrary quantities $f$ with respect to a high-dimensional distribution $P$, $\langle f \rangle = \int \! P(x) d^{d} x \, f(x)$. This is related to the problem of simply generating samples from $P$, because if we can do that we can estimate  $\langle f \rangle$ by evaluating $f$ there. 

An essential point of MC is that the error in doing so goes as $\var(f) / N$, and is \emph{independent} of the dimensionality $d$ of $x$. 

\subsubsection{Importance sampling} 
If we have an approximation $Q(x)$ to $P(x)$ that's cheaper to evaluate, we can simply draw samples from $Q$ and weight them according to $w_{i} = P(x_{i}) / Q(x_{i})$. Problems are 1) this is essentially uncontrolled without some guarantee on how close $Q$ is to $P$, and 2) suffers in high dimensions: weights become dominated by large values.

\subsubsection{Rejection sampling} 
Similar, but now assume we know a constant $c$ such that $cQ(x) > P(x)$ for all $x$. For each sample, generate a uniform variate $u_{i}$ from $0$ to $cQ(x_{i})$ and only keep the sample if $u_{i} < P(x)$. Retained samples are independent from P(x). Problem is finding $c$ so that rejection isn't too frequent, also harder in high dimensions.

\subsection{Markov chain Monte Carlo} 
Ref: \cite{MacKay:03} ch. 29.

Coming up with a single approximate $Q(x)$ is too hard, so instead make it dependent on the value of the last sample: $Q(x| x_{t})$ (now using $t$ to index steps). This means the sampling process is a stateful Markov chain. In general, method involves designing a Markov chain in $x$-space such that $P(x)$ is the unique invariant measure (uniqueness requires irreducibility and ergodicity). \emph{Detailed balance} is another term for reversibility,
\be
T(x, x') P(x') = T(x', x) P(x)
\ee
for all $x, x'$. Implies $P$ is invariant, but not essential!

Transition matrices need to preserve $P$, i.e. $P(x') = \int \! dx \, T(x', x) P(x)$. Can build from ``base transitions'' by taking convex linear combos or by convolution. 

Potential disadvantages of MC approach:

\begin{enumerate}
\item Samples are no longer independent: $\text{Pr}(x_t) \sim P(x)$ for large $t$, but hard to tell how many steps are needed for convergence. Including dependent samples in average \emph{doesn't} bias estimates [proof?], but doesn't help, and makes error estimates harder.

\item Doesn't allow direct access to normalization factor/partition function $Z$, although ratios possible.

\item Non-Bayesian. Properly Bayesian MC would give distribution for our knowledge of the estimator which would only depend on the evaluations at the same points, not on the initial configuration of the MC or other implementation details; see \citep{GhahramaniRasmussen:03}.
\end{enumerate}

Matter of art whether to use one long chain (better convergence) or multiple short chains, restarted from different points (lower correlations; better chance to explore state space if $P$ is multimodal). 

\subsubsection{Metropolis-Hastings}
Generate new sample $x'$ from $Q(x' | x_t)$ and compute weight
\be
a = \frac{P(x')}{P(x_t)} \frac{Q(x_t | x')}{Q(x' | x_t)};
\ee
if $a \geq 1$, \emph{accept} the new $x'$ as $x_{t+1}$. If $a < 1$, accept with probability $a$ and reject with $1-a$, in which case keep $x_{t+1} = x_{t}.$ 
More concretely: let $Q$ be gaussian centered on $x_t$, then we're effectively doing a random walk with step size of order $\sigma_{Q}$. (If $Q$ symmetric in arguments, second ratio is $1$.)

Small steps ($\sigma_{Q}$) means slow convergence, while large steps (relative to scale of peaks/features of $P$) also means slow convergence due to frequent rejection. \marginnote{What about fractal, multimodal $P$?} \emph{But} these considerations don't get worse with high dimension (intrinsically), unlike importance and rejection sampling. ``Efficient''/practical MC methods basically deal with reducing the problems arising from random walk behavior in vanilla Metropolis-Hastings.

\subsubsection{Gibbs sampling} The special case of the above when $Q$ is $P$ conditioned on all other components of the sample: $Q(x | \text{state}) = P(x_{(i)} | x_{(j) \neq i})$. Individual components of $x$ are updated in deterministic order:
\begin{align*}
x_{t+1, (1)} &\sim P(x_{(1)} | x_{t, (2)}, x_{t, (3)}, \ldots) \\
x_{t+1, (2)} &\sim P(x_{(2)} | x_{t+1, (1)}, x_{t, (3)}, \ldots) \\
x_{t+1, (3)} &\sim P(x_{(3)} | x_{t+1, (1)}, x_{t+1, (2)}, \ldots)
\end{align*}
This is Glauber dynamics (single spin flips) in condensed matter. Motivated as a ``quick-and-dirty'' method.

\subsection{Efficient MCMC} Ref: \cite{MacKay:03} ch. 30.

\subsubsection{Hamiltonian Monte Carlo} Improve random walk convergence with gradient information: adding drift to random walk gives linear instead of square-root convergence. Assumes $P(x) \sim \exp -\beta E(x)$, and that gradients of $E$ are cheap. Do this by adding momentum term $p^{2}/2$ to $E(x)$ but retaining only the $x$ coordinates of generated samples. \marginnote{What if we can only afford sampling $b \cdot \partial_{x}E$ along a few vectors $b$?}

Two-step Gibbs sampling scheme: First sample $p$ (always accepted), then update $x$ and then $p$ according to
\be
\dot{x} = p; \qquad \dot{p} = - \partial_{x} E(x).
\ee
If numerics exact, this step should also always be accepted under Metropoplis, since $p^{2}/2 + E(x)$ a constant of motion. \marginnote{What about assigning fancier dynamics, like Nambu? Fictitious dynamics chosen so integrals of motion give efficient sampling.} On the other hand, dynamics need to be \textit{exactly} reversible: state space volume must be conserved, and if $(x,p) \to (x',p')$ is generated as a deterministic update, we must also generate $(x, -p)$ starting from $(x', -p')$.

\subsubsection{Simulated annealing} Introduce fictitious temperature $\beta$ conjugate to $E(x)$ and tune $\beta \searrow 1$. Can only couple $\beta$ to ``messy'' terms in $E$ to interpolate between distributions. Simulated tempering \cite{MarinariParisi:92} fixes biases from getting trapped in individual minima by making $\beta$ a dynamically updated variable, see also annealed importance sampling \cite{Neal:01}.

\subsubsection{MC as a communication channel} 

Ref: \cite{MacKay:03} 30.5. \marginnote{Unfortunately sketchy. Anyone followed up?} 

Sampling an $x$ from $P(x)$ consumes at least $\log 1/P(x)$ random bits (cf. \href{https://en.wikipedia.org/wiki/Arithmetic_coding}{arithmetic coding}). 

Ignore $Q$ updates, then all information about true $P$ communicated by sequence of binary accept/reject Metropolis moves. So rule of thumb: maximize ``information learned'' about $P$ by tuning acceptance probability to be about $1/2$ (max entropy). Efficient methods try to pick $Q$ to beat this ``one bit per trial'' bound. Note that even importance sampling potentially gives us more than one bit to work with (the full ratio $P/ cQ$.)

Evolutionary analogy: acceptance is mutated genome replacing old one. Not clear that genetic methods actually do this, though. 

\subsubsection{Exact sampling} 
Ref: \cite{MacKay:03} ch. 32, \cite{ProppWilson:96}. 

Addresses questions of Markov chain convergence to target distribution. ``Three ideas,'' following MacKay:
\begin{enumerate}
\item Markov chain coalescence: Due to finite memory, if two runs of a chain from different initial conditions but using the same random number generator \marginnote{Randomness as a channel. Is there a way to phrase this that's realization-independent?} hit the same value, all subsequent values will be identical: chain has \textit{forgotten} the difference in initial conditions.

\item Bounds on coalescence: Impractical to restart chain for all possible initial conditions. For practical use, look for a \textit{partial order} on configurations that's invariant under MC dynamics. Extrema under this order provide a bound: if they've coalesced, know all conditions ``between'' them have as well. \marginnote{Relax this to partial domains of attraction? Live with probabilistic estimate that chains have converged?}

``Summary states'' for non-attractive distributions (Huber 1998), (Harvey and Neal 2000): bounds don't have to be tight, or even physical trajectories of system. Example given uses \textit{partial} configurations. 

\item Coupling from the past: Coalescence is a distinguished event (depends on details of how the chain is designed), so coalescence doesn't immediately imply convergence. Instead start run at time $T_{0}$ in past and run up to present; \marginnote{Cf. random dynamical systems.} if coalescence hasn't happened, increase $T_{0}$. If it has, unique configuration at present is an exact sample. All runs are made with identical realization of random numbers at each time. 

\end{enumerate}

Other applications of coupled Markov chains --- gets into interacting particle systems, right?


\subsubsection{Extreme values and large deviations} 
\begin{notebox}
Above assumes that we're taking expectation of things that are smooth. What if we're interested in extreme values instead?
\end{notebox}
[...]


\section{Kernel methods}

\subsection{Kernel trick}

Want to apply linear techniques in nonlinear situations. Assume we can do so by mapping configurations $\mbf{x}$ to a higher (possibly infinite)-dimensional feature space $\varphi(\mbf{x})$; $\varphi$ will be nonlinear.

Because we want to use linear methods in the feature space, it will usually be the case that $\varphi$ will only enter in the form of a kernel, or Gram matrix of known samples:
\be
K(\mbf{x}, \mbf{y}) \equiv \langle \varphi(\mbf{x}), \varphi(\mbf{x}) \rangle; \qquad K_{ij} = K(\mbf{x}_{i}, \mbf{x}_{j}).
\ee
The ``kernel trick'' is that by working in terms of positive semi-definite $K(\mbf{x}, \mbf{y})$, we don't need to explicitly design, compute or store the feature map $\varphi(\mbf{x})$. 

Example: classification; want a hyperplane separating two classes in feature space, which corresponds to a curved (but still sharp) class boundary in configuration space.




\subsection{Kernel mean embedding of distributions}

\subsubsection{Reproducing kernel Hilbert spaces}

``Reproducing'' here refers to the evaluation map (``evaluate $f$ at $x$''); one-to-one with existence of kernel via $\langle f, K(\mbf{x}, \cdot) \rangle = f(\mbf{x})$. Hilbert spaces that aren't RKHS are somewhat contrived.

\href{https://en.wikipedia.org/wiki/Representer_theorem}{Representer theorems} state that RKHS representation is useful for generalized forms of ridge regression. Min-error $f$ takes the form of a linear combination of $K(x_{i}, \cdot)$. 


\subsubsection{Kernel embedding}

Closely analogous. For a random variable $X \sim P$ and a given kernel $K$, the mean embedding is
\be
\mu_{X} = \mbb{E}\left[ K(X, \cdot) \right] = \int \!dx \,  p(x)\varphi(x).
\ee
We then get the expectation of any function via $ \mbb{E}\left[f(X) \right] = \langle f, \mu_{X} \rangle$. Hope that embedding $\varphi(X)$ captures sufficient statistics about $X$. 

Empirical/sample estimator just $\widehat{\mu}_{X} = \tfrac{1}{N} \sum \varphi(x_{i})$. Can show that convergence is $\sim 1/\sqrt{N}$, independent of the feature space dimensionality, so in this sense these methods get around the curse of dimensionality. 

Joint distribution of $X,Y$ as $C_{XY} = \mbb{E}\left[\varphi(X) \otimes \varphi(Y) \right]$. 

Conditional distributions: let $C_{Y|X} = C_{YX}^{\phantom{-1}} C_{XX}^{-1}$. Assuming existence, conditional embedding is a series of feature vectors indexed by $x$:
\be
\mu_{Y|X=x} = C_{Y|X} \varphi(x) = \int \! dy \, p(y|x)\varphi(y).
\ee

\subsubsection{Induced properties}

Norm on RKHS gives us a notion of ``distance'' between distributions\marginnote{Asymmetry of $D_{KL}$ means probably impossible to choose $\varphi$ to reproduce info-th metrics, right?}, the maximum mean discrepancy
\be
\text{MMD}(P_{X}, P_{Y}) = || \mu_{X} - \mu_{Y} ||^{2} = \sup_{||f|| \leq 1} \mbb{E} \left[ f(X) \right] -  \mbb{E} \left[ f(Y) \right].
\ee

Likewise, measure of dependence/correlation
\be
\text{HSIC}(X,Y) = || C_{XY} - \mu_{X} \otimes \mu_{Y} ||^{2}.
\ee


\begin{notebox}
How does this improve over Monte Carlo? Presumably on strength of kernel embedding. To what extent do we need to know $\varphi$ explicitly for encoding/decoding, though?

\end{notebox}


\section{Kernel approximations}
Computationally tractable implementations of kernel methods basically boil down to dense matrix operations; for large data applications these are too slow, and we instead want to use reduced-rank approximations.

\subsection{Random features}
``Random'' in the sense that we regard ``feature space'' as being indexed by a random variable $\theta$; 
\be
K(x, x') = \int \! d\theta \, p(\theta) \varphi_{\theta}(x) \varphi_{\theta}(x').
\ee
In computations we use a Monte Carlo approximation of this integral by $M$ samples of $\theta$.

Question then arises of how to choose $\varphi_{\theta}$ to reproduce known kernel $K$. Partial answer for translation-invariant $K$ from 

\subsubsection{Random Fourier features}
Question then arises of how to choose $\varphi_{\theta}$ to reproduce known kernel $K$. Partial answer for translation-invariant $K$ from Bochner's theorem: take $\varphi_{a,b}(x) \propto \cos(a^{T}x + b)$, with $b$ uniform on $[0, 2\pi)$ and $a$ taken from an $N$-dimensional distribution. 

$a \sim$ Gaussian reproduces the RBF kernel; $a \sim t$-distribution reproduces the Matern(1/2) kernel (in limit $M \to \infty$).


\subsection{Gamblets}

\subsection{Neural networks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{The linear-Gaussian world}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Regression}



\section{Kalman filtering}
Ref: mostly \href{https://en.wikipedia.org/wiki/Kalman_filter}{wiki}, with notation changes.

\subsection{Bayesian state estimation}
\label{sec-bayesian-state-estimation}

\subsubsection{Single Bayes update}

Assume we have a Gaussian prior for an unchanging latent variable $X \sim \mcal{N}(\mbf{\mu}_0, \mbf{\Sigma}_0)$ updated with a noisy observation $Z = \mbf{H}X + \mbf{\eta}$, with noise $\eta \sim \mcal{N}(0, \mbf{\Gamma})$. We want to update
\be
P(X | Z = z) = \frac{P(Z=z | X)}{P(Z=z)} P(X).
\ee
From above, we have $Z | X \sim \mcal{N}(\mbf{H}X, \mbf{\Gamma})$. Know posterior will also be normally distributed, so assume normalization works out; collect terms in $\mbf{x}$ to read off parameters of $X | Z \sim \mcal{N}(\mbf{\mu}, \mbf{\Sigma})$. We have
\begin{equation}
\label{eq-kal-1}
\mbf{\Sigma}^{-1} = \mbf{\Sigma}_0^{-1} + \mbf{H}^{T} \mbf{\Gamma}^{-1} \mbf{H};
\end{equation}
Apply \href{https://en.wikipedia.org/wiki/Woodbury_matrix_identity}{Woodbury identity} to invert
\be
\mbf{\Sigma} = \mbf{\Sigma}_0 + \mbf{\Sigma}_0 \mbf{H}^{T} \mbf{S}^{-1} \mbf{H} \mbf{\Sigma}_0,
\ee
abbreviating
\be
\mbf{S} \equiv \left( \mbf{\Gamma} + \mbf{H} \mbf{\Sigma}_{0} \mbf{H}^{T} \right).
\ee
Then, collecting terms linear in $\mbf{x}$ and matching against the desired term $- \mbf{x}^{T} \mbf{\Sigma}^{-1} \mbf{\mu}$,
\begin{equation}
\label{eq-kal-2}
\mbf{\mu} = \mbf{\Sigma}\left( \mbf{\Sigma}_0^{-1} \mbf{\mu}_{0} + \mbf{H}^{T} \mbf{\Gamma}^{-1}\mbf{z} \right).
\end{equation}
Expanding,
\begin{align*}
\mbf{\mu} = \mbf{\mu}_0 - \mbf{\Sigma}_0 \mbf{H}^{T} \mbf{S}^{-1} \mbf{H} \mbf{\mu}_0 &+ \mbf{\Sigma}_0 \mbf{H}^{T} \left[ 1 - \mbf{S}^{-1} \mbf{H} \mbf{\Sigma}_{0} \mbf{H}^{T} \right] \mbf{\Gamma}^{-1} \mbf{z} \\
 = \text{''} &+ \mbf{\Sigma}_0 \mbf{H}^{T} \left[ \mbf{S}^{-1} \mbf{S} - \mbf{S}^{-1} \left( \mbf{S} - \mbf{\Gamma} \right) \right] \mbf{\Gamma}^{-1} \mbf{z} \\
  = \text{''} &+ \mbf{\Sigma}_0 \mbf{H}^{T} \mbf{S}^{-1} \mbf{z}.
\end{align*}

\subsubsection{More terminology}
Re-express the above results by introducing new terms. 

\emph{Innovation} is residual before update:
\be
\mbf{y}_0 = \mbf{z} - \mbf{H} \mbf{\mu}_0,
\ee
satisfying $\mbb{E}[\mbf{y}_0] = 0$ and $\cov(\mbf{y}_0) = \mbf{S}$. 

\emph{Kalman gain} defined as how much we use $\mbf{y}_0$ to correct $\mbf{\mu}_{0}$;
\be
\mbf{\mu} = \mbf{\mu}_{0} + \mbf{K} \mbf{y}_0.
\ee
In the filtering problem, can define the optimal Kalman gain as the one that minimizes the posterior residual $\mbf{y} = \mbf{z} - \mbf{H} \mbf{\mu}$. Unsurprisingly, the minimum mean-square error gain is what we got above via Bayes' rule, namely
\be
\mbf{K}^{\ast} = \mbf{\Sigma}_0 \mbf{H}^{T} \mbf{S}^{-1}.
\ee
In these terms, the Bayes update is
\begin{equation}
\mbf{\mu} = \mbf{\mu}_0 + \mbf{K}^{\ast} \mbf{y}_0 = (1 - \mbf{K}^{\ast} \mbf{H})\mbf{\mu}_0 + \mbf{K}^{\ast}\mbf{z}; \qquad \mbf{\Sigma} = (1 - \mbf{K}^{\ast} \mbf{H}) \mbf{\Sigma}_0.
\end{equation}

%\subsection{Differential formulation?}
%These forms suggest interpolation from old to new parameters as $\mbf{K}$ is turned on from $0$ to $\mbf{K}^{\ast}$.
%
%Unclear what the Bayesian interpretation of the continuous time Kalman filter (Kalman-Bucy) is. 
%
%The Bayesian paradigm is inherently discrete-time: we can't update $\mbf{\mu}$ until we learn the value of the latest $\mbf{z}$. However, in the current context (no control input) all $\mbf{z}$s are iid, and $\mu$ should be independent of the update order (prove this). If the $\mbf{z}$s are drawn from an \href{https://en.wikipedia.org/wiki/Infinite_divisibility_(probability)}{infinitely divisible} distribution, it would make sense to consider an ``infinitesimal update.''
%
%A related, more physical idea would be to have the noise covariance $\mbf{\Gamma}^{(i)}(t)$ for measurement $i$ start off infinitely broad at $t=0$ and converge to $\mbf{\Gamma}$ at $t=i$.


\subsubsection{Information filter}

Jumping back and forth between $\mbf{\Sigma}$ and $\mbf{\Sigma}^{-1}$ is cumbersome. Instead, remain in inverse space, defining
\be
\check{\mbf{\Gamma}} \equiv \mbf{H}^{T} \mbf{\Gamma}^{-1} \mbf{H}; \qquad \check{\mbf{z}} \equiv \mbf{H}^{T} \mbf{\Gamma}^{-1} \mbf{z}; 
\qquad \check{\mbf{\mu}} \equiv \mbf{\Sigma}^{-1} \mbf \mu; \qquad \check{\mbf{\Sigma}} \equiv \mbf{\Sigma}^{-1}. 
\ee
The update can be read off from \eqref{eq-kal-1} and \eqref{eq-kal-2}:
\begin{equation}
\check{\mbf{\mu}}_{n} = \check{\mbf{\mu}}_{0} + \sum_{j=1}^{n} \check{\mbf{z}}_j; \qquad \check{\mbf{\Sigma}}_n = \check{\mbf{\Sigma}}_0 + n \check{\mbf{\Gamma}},
\end{equation}
In these variables the update is simply additive, so we can write down the multiple-update solution.



\subsection{Adding dynamics}

Context is estimation of a hidden Markov model (\boldref{sec-hidden-markov-models}) for continuous quantities in discrete time. We have stochastic system state 
\be
\mbf{x}_{t} = \mbf{F}_{t} \mbf{x}_{t-1} + \mbf{B} \mbf{u}_{t} + \xi_{t},
\ee
with iid noise $\xi \sim \mcal{N}(0, \mbf{\Gamma}_{\xi})$, possibly depending on control input $\mbf{u}_{t}$.

[...]

\section{Control}

\subsection{General background on control}
Ref: \cite{KappenRuiz:16}.

Generically we have a system state $\mbf{x}$, control signal $\mbf{u}$ and known dynamics $\dot{\mbf{x}} = f[t, \mbf{x}(t), \mbf{u}(\mbf{x},t)]$. The \emph{representation problem} is that it's wasteful to compute optimal $\mbf{u}$ for all possible $\mbf{x}$.

If dynamics are deterministic, only need control input along optimal trajectory $\mbf{u}^{\ast}(t) = \mbf{u}(\mbf{x}^{\ast}(t), t)$. Example of \emph{open-loop} control, where we apply $\mbf{u}(t)$ regardless of what the state actually is. 

With noise (stochastic dynamics), this isn't sufficient: we need to know what input to apply if we're perturbed off the optimal trajectory, and hence $\mbf{u}$ has to depend on $\mbf{x}$ (\emph{closed-loop} control). Can improve via a \emph{linear feedback} controller: Taylor expand around $\mbf{x}^{\ast}(t)$ to linear order in dynamics and quadratic in control cost. Then we can solve everything, obtaining a controller that stabilizes $\mbf{x}^{\ast}(t)$ when weak noise is turned on. However, turning on noise perturbs the optimal trajectory from the noiseless result, so we can repeat the construction with this new $\mbf{x}^{\ast}(t)$. This is ``differential dynamic programming'' or ``iterative LQG.''

\emph{Model predictive control} addresses representation in a different way, by only computing $\mbf{u}(\mbf{x},t)$ for states $\mbf{x}$ as needed. At time $t$ solve the finite-horizon control problem for the interval  $[t, t+T]$, but only apply the control for a shorter time $[t, t+dt]$; then solve the finite-horizon problem again from the new state (``receding horizon.'') Not globally optimal, but more robust.

\begin{notebox}
Versus directed polymer in random medium? At fixed $\mbf{u}(\mbf{x},t)$, assume dynamics of $\mbf{x}$ follow from least-action. Then functional average (path integral) over all $\mbf{u}$, weighted by cost as $e^{-\beta R(\mbf{x},\mbf{u})}$, except we want quenched average.
\end{notebox}


\subsection{Linear-Quadratic regulator}

Ref: \href{https://en.wikipedia.org/wiki/Linear\%E2\%80\%93quadratic_regulator}{wiki}; \href{http://www.argmin.net/2018/02/08/lqr/}{this blog post}; first chapter of Bertsekas.

\subsubsection{Notation}

Simplest model of a linear feedback controller. Turn off noise for this section; system state $\mbf{x}_{t}$, control action $\mbf{u}_{t}$, time $t$ taken discrete. Start with finite-horizon; $t \in [0, T+1]$.

Take dynamics to be deterministic, linear and exactly known:
\begin{equation}
\label{eq-lqr-eom}
\mbf{x}_{t+1} = f(\mbf{x}_{t}, \mbf{u}_{t}) = \mbf{A}\mbf{x}_{t} + \mbf{B}\mbf{u}_{t},
\end{equation}
with boundary condition $\mbf{x}_{0}$. We want to find the control trajectory $\mbf{u}^{\ast}_{t}$ which is optimal in the sense of minimizing cost, assumed quadratic:
\be
G = \sum_{t} g(\mbf{x}_{t}, \mbf{u}_{t}) = \tfrac{1}{2} \mbf{x}^{T}_{T+1} \mbf{\Psi} \mbf{x}_{T+1} + \sum_{t=0}^{T} \tfrac{1}{2} \mbf{x}^{T}_{t} \mbf{Q} \mbf{x}_{t} + \tfrac{1}{2} \mbf{u}^{T}_{t} \mbf{R} \mbf{u}_{t}. 
\ee
Drop possible $\mbf{x}-\mbf{u}$ cross term in above in the name of simplicity; for the same reason matrices could be $t$-dependent, but we suppress this. Crunch through solution first, then return to question of what conditions we need on all these matrices. 

\subsubsection{Via dynamic programming (Bellman)}

``Bellman optimality principle,'' and starting point for applicability of dynamic programming, is that sub-trajectory of an optimal trajectory is itself optimal (assuming convexity, or more broadly absence of \emph{topological} obstructions).

Define ``cost-to-go'' $J_{t}(\mbf{x}_{t})$ as cost of sub-trajectory on $[t,T+1]$, starting from initial condition $\mbf{x}_{t}$. Careful on notation: $J_{t}$ has time dependence beyond that from $\mbf{x}_{t}$; also implicit dependence on rest of trajectory of $\mbf{x}, \mbf{u}$.

\emph{Optimal} cost-to-go refers to optimizing with respect to $\mbf{u}_{t}$. This is done \emph{backwards} in time:
\begin{equation}
\label{eq-lqr-bell1}
J^{\ast}_{t}(\mbf{x}_{t}) = \min_{\mbf{u}_{t}} \left[ g(\mbf{x}_{t}, \mbf{u}_{t}) + J^{\ast}_{t+1}\left( f(\mbf{x}_{t}, \mbf{u}_{t}) \right) \right]
\end{equation}
``Arrow of time'' fixed by need to substitute $f(\mbf{x}_{t}, \mbf{u}_{t})$ for $\mbf{x}_{t+1}$. Boundary condition is set in future, at $T+1$, because no control action in last step.

So $J^{\ast}_{t}(\mbf{x}_{t})$ is like a time-dependent \emph{potential}: need to maintain $\mbf{x}_{t}$ as a free variable when solving. Need to retain full functional dependence essentially the main obstacle with reinforcement learning, and motivation for introducing approximation methods.


Boundary condition is $J^{\ast}_{T+1}(\mbf{x}_{T+1}) = \tfrac{1}{2} \mbf{x}^{T}_{T+1} \mbf{\Psi} \mbf{x}_{T+1}$; we take this as an ansatz and show that if we have $J^{\ast}_{t+1}(\mbf{x}_{t+1}) = \tfrac{1}{2} \mbf{x}^{T}_{t+1} \mbf{M}_{t+1} \mbf{x}_{t+1}$ for some matrix $ \mbf{M}_{t+1}$, this form will be preserved under the backwards recursion, i.e. it implies $J^{\ast}_{t}(\mbf{x}_{t}) = \tfrac{1}{2} \mbf{x}^{T}_{t} \mbf{M}_{t} \mbf{x}_{t}$ for some other $\mbf{M}_{t}$.

Variation with respect to $\mbf{u}_{t}$ in \eqref{eq-lqr-bell1} gives
\be
0 = \mbf{R} \mbf{u}^{\ast}_{t} + \mbf{B}^{T} \mbf{M}_{t+1} \mbf{A} \mbf{x}_{t} + \mbf{B}^{T} \mbf{M}_{t+1} \mbf{B} \mbf{u}^{\ast}_{t},
\ee
so
\be
\mbf{u}^{\ast}_{t} = - \left[ \mbf{R} + \mbf{B}^{T} \mbf{M}_{t+1} \mbf{B} \right]^{-1} \mbf{B}^{T} \mbf{M}_{t+1} \mbf{A} \mbf{x}_{t} \equiv - \mbf{K}_{t+1} \mbf{x}_{t}.
\ee
This lets us eliminate $\mbf{u}_{t}$ in favor of $\mbf{x}_{t}$ on the RHS of \eqref{eq-lqr-bell1}, so we can see that the ansatz for the form of $J_{t}$ is preserved. Explicitly,
\be
\mbf{M}_{t} = \mbf{Q} + \mbf{A}^{T} \mbf{M}_{t+1} \mbf{A} + \mbf{K}^{T}_{t+1} \left[ \mbf{R} +  \mbf{B}^{T} \mbf{M}_{t+1} \mbf{B} \right] \mbf{K}_{t+1} - \left( \mbf{A}^{T} \mbf{M}_{t+1} \mbf{B} \mbf{K}_{t+1} + \mbf{K}_{t+1}^{T} \mbf{B}^{T} \mbf{M}_{t+1} \mbf{A}  \right),
\ee
but expanding the third term shows that it's equal to half the term in parentheses, so we obtain the discrete-time algebraic Riccati equation:
\begin{equation}
\label{eq-lqr-dare}
\mbf{M}_{t} = \mbf{Q} + \mbf{A}^{T} \mbf{M}_{t+1} \mbf{A} -  \mbf{A}^{T} \mbf{M}_{t+1} \mbf{B}\left[ \mbf{R} +  \mbf{B}^{T} \mbf{M}_{t+1} \mbf{B} \right]^{-1} \mbf{B}^{T} \mbf{M}_{t+1} \mbf{A}.
\end{equation}
If we make the further assumption that $\mbf{M}$ is invertible, we can apply the \href{https://en.wikipedia.org/wiki/Woodbury_matrix_identity}{Woodbury identity}
\be
\left[ \mbf{A} + \mbf{U}^{T} \mbf{C} \mbf{U} \right]^{-1} = \mbf{A}^{-1} - \mbf{A}^{-1}\mbf{U}^{T} \left[ \mbf{C}^{-1} + \mbf{U} \mbf{A}^{-1} \mbf{U}^{T} \right]^{-1} \mbf{U} \mbf{A}^{-1}
\ee
to obtain
\begin{equation}
\label{eq-lqr-dare2}
\mbf{M}_{t} = \mbf{Q} + \mbf{A}^{T}\left[  \mbf{M}_{t+1}^{-1} + \mbf{B} \mbf{R}^{-1}\mbf{B}^{T} \right]^{-1} \mbf{A}.
\end{equation}



\subsubsection{Via Pontryagin maximum principle}

Constrain dynamics to be physical\marginnote{cf. on-shell vs. off-shell when we move to path integral?} though Lagrange multipliers (``costates'' or ``adjoints'') $\mbf{\lambda}_{t}$: $L = G - \mbf{\lambda}_{t+1}^{T}(\mbf{x}_{t+1} - \mbf{A}\mbf{x}_{t} - \mbf{B}\mbf{u}_{t} )$. Find optimality at extrema of Lagrangian; we have
\begin{align}
\label{eq-lqr-lag1} \text{\href{https://en.wikipedia.org/wiki/Costate_equation}{Costate equation: }} 0 &= \frac{\delta L}{\delta \mbf{x}_{t}} \Rightarrow \mbf{\lambda}_{t} = \mbf{A}^{T} \mbf{\lambda}_{t+1} + \mbf{Q} \mbf{x}_{t}, \\
\label{eq-lqr-lag2} \text{Boundary condition: } 0 &= \frac{\delta L}{\delta \mbf{x}_{T+1}} \Rightarrow \mbf{\lambda}_{T+1} = \Psi \mbf{x}_{T+1}, \\
\label{eq-lqr-lag3} \text{Stationary equation: } 0 &= \frac{\delta L}{\delta \mbf{u}_{t}} \Rightarrow \mbf{u}_{t} = - \mbf{R}^{-1} \mbf{B}^{T} \mbf{\lambda}_{t+1}, \\
\label{eq-lqr-lag4} \text{State equation: } 0 &= \frac{\delta L}{\delta \mbf{\lambda}_{t}} \Rightarrow \mbf{x}_{t+1} = \mbf{A}\mbf{x}_{t} + \mbf{B}\mbf{u}_{t}.
\end{align}


Similar to previous section, we find the costate equation describes evolution of $\lambda_{t}$ backwards in time from a boundary condition at $T+1$. 

Stationary equation \eqref{eq-lqr-lag3} can viewed as (related to?) a Legendre transform, exchanging $\mbf{u}$ for $\lambda$ as dependent variable. Eliminating $\mbf{u}_{t}$ from \eqref{eq-lqr-lag1}, \eqref{eq-lqr-lag4} gives joint recursion
\be
\begin{pmatrix} \mbf{x}_{t+1}  \\ \lambda_{t} \end{pmatrix} = \left[ \begin{matrix} \mbf{A} & - \mbf{B} \mbf{R}^{-1} \mbf{B}^{T} \\ \mbf{Q} & \mbf{A}^{T} \end{matrix} \right] \begin{pmatrix} \mbf{x}_{t}  \\ \lambda_{t+1} \end{pmatrix}.
\ee

Solution proceeds similarly, taking boundary condition as ansatz, and showing that if $\lambda_{t+1} = \mbf{M}_{t+1} \mbf{x}_{t+1}$ for some $\mbf{M}_{t+1}$, this dependence is preserved. Use notation $\mbf{M}$ because we obtain same discrete-time Riccati equation as above.

\begin{notebox}
Is there something deeper going on here? A priori, could say that $\lambda_{t}$ depends on future trajectory of $\mbf{x}$s, while $\mbf{x}_{t}$ depends on history of $\mbf{x}$s, so ``intersection'' is to have $\lambda_{t}$ and $\mbf{x}_{t}$ depend on each other only at that value of $t$.
\end{notebox}

Substituting in ansatz, we get
\be
\mbf{x}_{t+1} = \left[ 1 + \mbf{B} \mbf{R}^{-1} \mbf{B}^{T} \mbf{M}_{t+1} \right]^{-1} \mbf{A} \mbf{x}_{t},
\ee
so
\be
\lambda_{t} = \left\{ \mbf{Q} + \mbf{A}^{T} \mbf{M}_{t+1}\left[ 1 + \mbf{B} \mbf{R}^{-1} \mbf{B}^{T} \mbf{M}_{t+1} \right]^{-1} \mbf{A} \right\} \mbf{x}_{t}
\ee
Quantity in curly brackets is $\mbf{M}_{t}$; inserting $1 = \mbf{M}_{t+1}^{-1} \mbf{M}_{t+1}$ gives us the discrete-time Riccati equation in the form \eqref{eq-lqr-dare2}.


\begin{notebox}
Relationship to Bellman? Write up relationship between Pontryagin and HJB. Is this a substantive change in what we're solving for --- i.e. can we take $\lambda_{t}$ to be a field, or is it indirect bookkeeping for the potential $J^{\ast}_{t} (\mbf{x}_{t})$ --- seems like we need to retain $\mbf{x}_{t}$-dependence, as with form of ansatz.

Are we just doing $\lambda_{t} = \delta_{\mbf{x}_{t}} J^{\ast}_{t} (\mbf{x}_{t})$?
\end{notebox}


\subsubsection{Infinite-horizon problems}

For $T \rightarrow \infty$, physical solutions to the above are steady states, so fixed points of the Riccatti equation $\mbf{M}_{t+1} = \mbf{M}_{t} = \mbf{M}$.





\subsection{Linear-Quadratic control with Gaussian noise}
[...]

Linearity means zero-mean, IID noise may be absorbed into a change of variable: $\mtilde{x}_{t} = x_{t} - e_{t-1}$. Then $\left\langle \mtilde{x} Q \mtilde{x} \right\rangle_{e} = xQx + \left\langle e Q e \right\rangle_{e}$. Maybe better to assume noiseless, then show adding noise doesn't create substantial problems: Achievable \emph{reward} degrades, but the optimal control signal $u$ is identical to the noiseless case.


\subsection{Control vs. ML}
\label{sec-control-vs-ml}
Ref: \cite{NiEtAl:21}. 

(PO)MDPs are problem contexts, while RL refers to a family of algorithms for solving related problems.
\begin{itemize}
\item MDP assumes all dynamics is known to the agent at outset, and we only need to learn a policy. ``For any MDP, there exists an optimal policy that is both memoryless and deterministic.'' \marginnote{Even with stochastic dynamics?} 
\item POMDP assumes only part of the MDP state is accessible to the agent, and is revealed through observations.
\item RL assumes dynamics are knowable/fixed, but must be learned by the agent through exploration; it's the problem of learning a fixed MDP (either entirely online, or with offline data).
\end{itemize}
In addition, dynamics and observations may be deterministic or stochastic. 

POMDP formally reducible to MDP: replace partial knowledge of real state with exact knowledge of ``belief state,'' the probability of the real state given the full history of observations up to that point. This obviously invokes the curse of dimensionality.

Conversely, an MDP with uncertainty in its parameters can be modeled as a POMDP. ``Bayesian RL'' is RL done in the context of a belief distribution over the underlying MDP. Of course, this is intractable.\marginnote{Harder or easier than general POMDP?}

\cite{GhoshEtAl:21} frames poor generalization of RL from in-sample/offline training in terms of an ``epistemic POMDP.''


\begin{table}[h!]
\centering
\begin{tabularx}{0.95\textwidth} { 
>{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X
}
\hline\hline
Sub-area & $s'$ in dynamics? & $s'$ in reward? & $s'$ constant? & Policy inputs & RL objective & Domain shift? \\
\hline\hline
Standard POMDP & Y & Y & N & $O, A, R$ & Avg & N \\ 
\hline
Meta RL & $\sim$N & Y & Y & $O, A, R, d$ & Avg & N \\ 
\hline
Robust RL & $\sim$Y & $\sim$N & $\sim$Y & $O, A$ & Worst & N \\ 
\hline
Gen'lization in RL & $\sim$Y & $\sim$N & $\sim$Y & $O, A$ & Avg &  $\sim$Y \\ 
\hline\hline
\end{tabularx}
\caption{From \cite{NiEtAl:21}. $s'$ refers to the hidden POMDP state; $O, A, R, d$ refer to the sequence of observations, actions, rewards, and done signals, respectively. $\sim$ means the categorization doesn't hold for all work.
}
\end{table}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Continuum formulations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Probability}


\subsection{Gaussian Processes}

[...]


\subsection{Lagrangians}
(\href{http://math.mit.edu/classes/18.086/2006/am72.pdf}{ref} for this (Strang)). 

Three forms for all calculus of variations problems:
\begin{enumerate}
\item Variational (optimization; here least action principle):
\be
\min_{x} \mcal{S}[x] = \min_{x} \left\{ \int \! dt \, \mcal{L}[t, x(t), \dot{x}(t)] \right\},
\ee with Dirichlet BCs on endpoints $x(0)$, $x(T)$. 
\item Weak form (here ``principle of virtual work''). Take any test function $y$, form $\mcal{S}[x+y] - \mcal{S}[x]$ and set term linear in $y$ to zero to express optimality. 
\be
\delta \mcal{S} = \int dt \, \left[ \frac{\partial \mcal{L}}{\partial x} \delta x  + \frac{\partial \mcal{L}}{\partial \dot{x}} \delta\dot{y} \right]= 0.
\ee
\item Strong form (here Euler-Lagrange equations): Integrate weak form by parts. Boundary conditions handle surface term.
\be
\frac{\partial \mcal{L}}{\partial x} - \frac{d}{dt} \frac{\partial \mcal{L}}{\partial \dot{x}} = 0.
\ee
\end{enumerate}
Even simpler matrix case (to motivate KKT et al): $\min_{u} \tfrac{1}{2} u' A u - u' b$; then $v' A u = v' b$ for any $v$, or $Au = f$.


\section{Control}

\subsection{Bellman 2}
(\href{https://math.stackexchange.com/questions/782621/difference-between-variation-of-calculus-problems-and-control-theory-problems}{ref} for this, also wiki). 

Switch notation. Control theory only fixes initial endpoint (just adding additional bookkeeping for optimization over final BC, right?). Also deals with differential constraints, so introduce $u$ as a multiplier for $\dot{x}$ and you'd say
\begin{align*}
\min_{u} \mcal{S}[u] &= \min_{u}\left\{ \Phi[x(T)] + \int_{0}^{T} dt \, \mcal{F}[t, x(t), u(t)] \right\}, \\
\dot{x} &= g[t, x(t), u(t)].
\end{align*}
where the second equation (first-order dynamic constraints) is called the ``state equation'' and $\Phi$ is the ``endpoint cost.'' Can also have ``path constraints'' on $x$, $u$ (no derivatives).

\emph{Pontryagin's Maximum Principle} just buys us the ability to deal with discontinuity? Replace $\delta \mcal{H} = 0$ condition with simple $\max \mcal{H}$?



\subsection{Path integral control}
Ref: \cite{TheodorouEtAl:10}.

Finite horizon stochastic control. Make assumption that dynamics \emph{linear} in $u_{t}$, reward is \emph{quadratic} in $u_{t}$:
\be
\dot{x}_{t} = f(x_{t},t) + g(x_{t})[u_{t} + dw_{t}]; \qquad r_{t} = q(x_{t},t) + \frac{1}{2} u_{t}^{T}R u_{t},
\ee
where Gaussian noise $dw$ has variance $\Sigma_{w}$. HJB equation for cost-to-go is then
\be
-\partial_{t} J(x_{t}, t) = \min_{u} \left[ r_{t} + (\partial_{x} J)^{T} (f_{t} + g_{t}u_{t}) + \frac{1}{2} \Tr g_{t} \Sigma_{w} g_{t}^{T} \right],
\ee
which is solved by
\be
u^{\ast}(x_{t}) = -R^{-1} g_{t}^{T} \partial_{x} J.
\ee
Substituting this back into HJB gives a nonlinear PDE for $J$, which may be linearized by the change of variables $J = - \lambda \log \Psi(x_{t}, t)$. \marginnote{How does this relate to Schrodinger/physical notions? $\lambda$ like $\hbar$...} 

Crucial point for this method: We need to assume $\lambda R^{-1} = \Sigma_{w}$, which is expressing the observation that variance of the control input and cost of that input are inversely related. (This is dictated by the need for linearization and isn't a well-motivated assumption in general: it implies that noiseless variables can't be controlled [and vice versa?]).

Under this assumption we get the Chapman-Kolmogorov PDE
\be
-\partial_{t} \Psi = - \frac{1}{\lambda} q_{t} \Psi + f_{t}^{T} \partial_{x} \Psi + \frac{1}{2} \Tr \, (\partial^{2}_{x} \Psi) g_{t} \Sigma_{w} g_{t}^{T},
\ee
with boundary condition $\Psi(t_{F}) = \exp [ - \phi(t_{F}) / \lambda]$. This can be expressed as a path integral using the Feynman-Kac theorem:
\be
\Psi( x_i, t_i) = \int \! d\xi \, \exp - \frac{1}{\lambda} \left( \phi(t_{F}) + \int_{t_i}^{t_{F}} \! dt \, q_{t} \right),
\ee
where the integration is over all trajectories $\xi$ starting at $x_{i}(t_i).$

Theodorou et al. go on to generalize to the case where $g_{t}$ is state-dependent and partitioned into controlled [$(c)$] and non-controlled degrees of freedom. Define generalized cost
\begin{align*}
\widetilde{S}(\xi) &= S(\xi) + \frac{\lambda}{2} \int_{t_i}^{t_{F}} \! dt \, \log |H(t_{j})|, \text{ where} \\
S(\xi) &= \phi(t_{F}) + \int_{t_i}^{t_{F}} \! dt \, \left( q_{t} + || \dot{x}^{(c)}_{t} - f^{(c)}_{t} ||^{2}_{H_{t}^{-1}} \right); \\
H_{t} &= g^{(c)}_{t} R^{-1}g^{(c) T}_{t}.
\end{align*}
$\widetilde{S}(\xi)/\lambda$, normalized by the associated partition function $\widetilde{Z}$, is the path integral probability measure for the path $\xi$. \marginnote{How does this compare to e.g. MSR for the uncontrolled Langevin dynamics? What would diagrams look like?} The optimal control can be written as an expectation with respect to it: 
\be
u^{\ast} = \frac{1}{\widetilde{Z}} \int \! d\xi \, e^{-\frac{\widetilde{S}}{\lambda}} R^{-1} g^{(c) T}_{t} H_{t}^{-1} \left[ g^{(c)}_{t} dw_{t} - \frac{\lambda}{2} H_{t} \Tr \, (H_{t}^{-1} \partial_{x} H_{t}) \right].
\ee 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Variational/Bayesian autoencoders.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Information bottleneck}

\subsection{Overview}
Ref: \cite{TishbyEtAl:99}

Given a source $X$ and target $Y$, we want a ``compressed representation'' $T$ that preserves ``only the information about $X$ relevant for $Y$.'' 

In more detail, assume $T \rightarrow X \rightarrow Y$ is Markov. In other words, we have a factorization assumption
\be
p(X,T,Y) = p(T|X,Y)p(Y|X)p(X) = p(T|X)p(Y|X)p(X).
\ee
The fact that $p(T|X,Y) = p(T|X)$ means $T$ can't ``look directly at the labels'' in $Y$. Then we want 
\begin{itemize}
\item $\min I(T;X)$ to minimize complexity, and
\item $\max I(T;Y)$ to maximize accuracy.
\end{itemize}
This motivates the info bottleneck Lagrangian:
\be
\mcal{L}_{IB} = I(X;T) - \beta I(Y;T).
\ee
[note that literature differs in minimization vs. maximization, and which term has the $\beta$.] This is implemented in terms of stochastic encoding and decoding functions, $p_{\text{enc}}(t|x)$ and $p_{\text{dec}}(y|t)$ respectively. The former is what's minimized when we minimize $\mcal{L}_{IB}$; for the present case, the latter is fully defined in terms of it via the Markov/factorization assumption:
\be
p_{\text{dec}}(y|t) = \sum_{x} p(x,y|t) = \sum_{x} p(y|x)p(x|t) = \sum_{x} p(y|x)\frac{p_{\text{enc}}(t|x)p(x)}{p_{\text{enc}}(t)}.
\ee

As formulated, $\mcal{L}_{IB}$ is non-convex, making optimizing $p_{\text{enc}}$ difficult.


\subsection{Sufficient dimensionality reduction}
Ref: \cite{GlobersonTishby:03}

Considers learning continuous \emph{features}: a regression problem, rather clustering. Original IB formulated in terms of discrete variables, for which this distinction not really present; the fact that the IB objective involves mutual information only means that it's invariant under reparameterizations, but this is important in practice.

Formulate regression as a sufficient statistics problem; learning $y = f(x) = \langle x \rangle_{p(x|y)}$. ``Feature extraction'' as functions $\phi(x)$ of one variable which are maximally informative with respect to other variables. Let $\mathcal{P}(\phi)$ be space of joint distributions $\widetilde{p}(x,y)$ having same marginals and $\langle \phi(x) \rangle$ as the real $p(x,y)$. ``Info in measurement $\phi$'' is 
\be
I_{\text{meas}}( \phi; p) = \min_{\mathcal{P}(\phi)} I(X;Y) = \max_{\mathcal{P}(\phi)} H(X,Y) + \text{const}., 
\ee
Follows this uniquely achieved by the exponential $\widetilde{p}(x,y) \propto \exp \left[\lambda_{X}(x) + \lambda_{Y}(y) + \sum_{i} \lambda_{i}(y) \phi_{i}(x)\right]$, where $\{\lambda\}$s are Lagrange multipliers, all of which depend on the choice of $\{\phi\}$. Claim we find optimal $\widetilde{p}^{\ast}(x,y)$ from minimizing $D_{KL}[p | \widetilde{p}^{\ast}]$, restricted to this exponential form (recall that sufficient statistics exist iff distribution belongs to an exponential family; this is just trying to find the best-fit exponential for $p$.)


\begin{itemize}
\item Problem actually has a symmetry $X \leftrightarrow Y$, $\phi_{i}(x) \leftrightarrow \lambda_{i}(y)$. 
\item Information-geometric interpretation of all this. 
\item ``Most informative features'' $\phi^{\ast}(x)$ maximize $I_{\text{meas}}( \phi; p)$. How to find systematically?
\item Can incorporate ``side information'' in the form of other variables that we want features to be \emph{un}informative about \cite{GlobersonEtAl:12}, by adding term to objective function with opposite sign.
\item How does this differ from a vanilla variational autoencoder? \cite{BanijamaliEtAl:18} seek to minimize objective 
\be
\langle \log q(x|t) + \log q(y|t) \rangle_{q(t|x)} - D_{KL}[q(t|x) | q(t)]
\ee
for NN encoder $q(t|x)$, decoder $q(x|t)$ and classifier $q(y|t)$.
\end{itemize}


[...]

\subsection{Tishby NIPS 2011 tutorial.} Ref: video on \href{https://www.youtube.com/watch?v=GKm53xGbAOk}{youtube}

\subsubsection{Sufficient statistics}
Bayesian hypothesis testing. Given samples $\mbf{x} = \{x_{i}\}$, determine which distribution $\omega_{j}$ they came from (start with distinguishing between two distributions.) Write the information gain $\Delta$ about $\omega$ provided by $\mbf{x}$ as
\begin{align*}
\Delta(\omega|\mbf{x}) = \frac{p(\omega|\mbf{x})}{p(\omega)} &= \frac{p(\mbf{x}|\omega)}{p(\mbf{x})} = \sum_{j}p(\mbf{x}|\omega_{j}); \\
\text{rewrite as}   &= \left( 1+ \exp \sum_{i} \log \frac{p(\mbf{x}|\omega_{2})}{p(\mbf{x}|\omega_{1})} \right)^{-1} \\
&= - \log \left( 1 + \exp \sum_{i} \log \frac{p(\mbf{x}|\omega_{2})}{p(\mbf{x}|\omega_{1})} \right).
\end{align*}
In what follows, define $T(\mbf{x}) = \sum_{i} \log \frac{p(\mbf{x}|\omega_{2})}{p(\mbf{x}|\omega_{1})}$. This is an additive function of the samples $\{x\}$ only.

Fisher-Neyman factorization: can factorize the \emph{joint} distribution as $p(\mbf{x},\omega) = f(\mbf{x})g(\omega, T(\mbf{x}))$, meaning $T(\mbf{x})$ is a sufficient statistic for $\omega$. No matter how many samples we have, all that matters is the single number $T(\mbf{x})$. 

Since $T(\mbf{x})$ is a sum of IID terms, the law of large numbers says 
\be
\frac{1}{N} \sum^{N} \log \frac{p(\mbf{x}|\omega_{2})}{p(\mbf{x}|\omega_{1})} 
\underset{N\rightarrow\infty}{\longrightarrow}
\left\langle \log \frac{p(\mbf{x}|\omega_{2})}{p(\mbf{x}|\omega_{1})} \right\rangle_{p(\mbf{x},\omega)}.
\ee
through ``typicality'': asymptotics are dominated by the average logs. This is basically the KL divergence
\be
0 \leq D[\omega_{1}|\omega_{2}] = \left\langle \log \frac{\omega_{2}(\mbf{x})}{\omega_{1}(\mbf{x})} \right\rangle_{\omega_{2}(\mbf{x})}.
\ee

\emph{Expected} info gain is just the mutual information: $\langle \Delta(\omega|\mbf{x}) \rangle_{p(\mbf{x}, \omega)} = I(X; \Omega) \geq 0$. Note that $T(\mbf{x})$ grows linearly with $N$ (extensive), while $I$ is subextensive: $\sim \log N$ for a parametric distribution (Cramer-Rao bound on parameter estimation) or $\sim N^{\eta}$ for $0 < \eta < 1$ for a distribution that's nonparametric but still ``learnable'', cf. minimal description length. 

To summarize, this illustrates connections between
\begin{itemize}
\item Learning with a binary hypothesis (need more than one $T(\mbf{x})$ to distinguish between more than two distributions),
\item Making an optimal binary decision,
\item The optimal linear discriminator (claim here a perceptron on a simplex).
\end{itemize}

Minimal sufficient statistics are great but exist \emph{only} for exponential families (= Gibbs states?). ML tries to be distribution independent. Recall a sufficient statistic $T$ for a hypothesis $\theta$ satisfies $p(\mbf{x}|T,\theta) = p(\mbf{x}|T)$. A \emph{minimal} sufficient statistic is an (algebraic?) function of any other sufficient statistic (or vice versa?): it's the coarsest possible ``partition'' of sample space.

Exponential families: Pitman, Koopman, Darmois.
\be
p(x | \theta) = h(x) \exp \left[ \sum_{r} \eta_{r}(\theta) A_{r}(x) - A_{0}(\theta) \right];
\ee
the maximum-entropy distribution subject to the constraints defined by the $\{ A_{r}\}$. Then sufficient statistics are $T_{r}(\mbf{x}) = \sum_{i} A_{r}(x_{i})$; additive for IID samples.

\subsubsection{Info bottleneck}
Motivate info bottleneck as the appropriate generalization of sufficient statistics: for the case of exponential families, IB recovers sufficient statistics.

Claim we go beyond the formalism of exponential families and sufficient statistics with \emph{mutual information}, as ``the maximum number of independent bits about $Y$ that can be given by measurement of $X$.'' Can define MI as the unique measure satisfying both:
\begin{enumerate}
\item Data processing inequality: if $X \rightarrow Y \rightarrow Z$ is Markov then $I(X;Z) \leq I(X;Y)$: a feedforward process can't increase information. Many measures other than $I$ obey this, cf. Renyi entropy, Chisaeu (sp?) divergences.
\item Bregman divergences: averaging inequality.
\end{enumerate}

Sufficiency and information. Bayesian approach: take $\theta$ random. Then $T$ is sufficient for $\theta$ iff $I(T;\theta) = I(X, \theta)$. $S$ is minimally sufficient if it retains the \emph{least} MI: for any $T$, $I(S;X) \leq I(T;X)$.

\subsubsection{Discrete case} 
Info bottleneck for clustering: original method proposed in \cite{TishbyEtAl:99}. Self-consistent equations
\begin{align*}
p(t|x) &= \frac{p(t)}{Z} \exp -\beta D_{KL}[p(y|x) | p(y|t)], \\
p(t) &= \sum_{x} p(t|x) p(x), \\
p(y|t) &= \sum_{x} p(y|x) p(x|t).
\end{align*}
Propose solving this iteratively using the first equation to update $p_{\text{new}}(t|x)$, but this is nonconvex. Arimoto-Blahut algorithm: alternating ``$I$-projection'' on three convex sets (left-hand sides of above). Proved technical results on convergence from empirical samples, uniqueness, optimality. 

\subsubsection{Gaussian case} 

Ref: \cite{ChechikEtAl:05}. 

Recover canonical correlation analysis. Bottleneck $T$ is a combination of CCA eignevectors: $T=AX$ where
\be
A = [\alpha_{1} \mbf{u}_{1}, \ldots, \alpha_{n} \mbf{u}_{n}]; \qquad (\Sigma_{XY} \Sigma^{-1}_{XX}) \mbf{u}_{k} = \lambda_{k} \mbf{u}_{k},
\ee
and $\alpha_{k}^{2} = \max[0, (\beta(1-\lambda_{k}) -1 )/\lambda_{k}]$. The $\max[\ldots]$ produces discrete structural transitions: $\beta$ sets the rank of $A$. cf. Shannon ``water filling'' analogy. IB tradeoff curve can be obtained analytically in terms of the $\{ \lambda_{k} \}$. 

Remark that for self-similar data, the $\{ \lambda_{k} \}$ satisfy a recursion and the IB curve is a power law. How deep is the recursion? How to determine empirically? What about multifractal processes??

\subsubsection{Kernel IB} Jacoby and Tishby 2011. ``When things aren't Gaussian, make them Gaussian'': Embed data (nonlinearly) in a sufficiently high-dimensional space with the ``kernel trick,'' then hope linear analysis on that works. Same method used in support vector machines, kernel PCA, kernel CCA. Means choice of embedding kernel is key.

Q: what if $X$, $Y$ don't have finite second moments?

\subsubsection{Predictive information and control} Estimation and control $\rightarrow$ compression and prediction. Not all info from the past is usable for predicting the future. Want to find this (cf. rate distortion coding) and perform a past-future IB. Past info as a ``perception channel'' and future state as a ``prediction channel.'' ``We see what we expect to see''--- perception guided by prediction. Coarse grained variables are predictable further into the future.

Partially-observed Markov decision process: add a hidden Markov model to a MDP. Hidden state of world $W$, observed state $M$, observation channel $O$ and action $A$. Claim that if observations reveal full state of the world, we're back to an MDP and memory isn't needed. 

Reinforcement learning: assign reward to each transition in world $W_{t} \rightarrow W_{t+1}$. Also introduce an ``intrinsic reward'' for uncovering more info from observations $M_{t} \rightarrow M_{t+1}$. Switch to discrete setting for tractability: stochastic MDP defines states $s$, actions $a$, transitions $p(s'|s,a)$. Stochasticity because we can't be certain what state we're in. 

Planning problem: want optimal policy $\pi(a|s)$ maximizing expected future reward. Bellman optimality: see Emo Todorov, Bert Kappen, Karl Friston. 


\subsection{Recent work on info bottleneck.}


\subsubsection{Related work and follow-ups}

\cite{GoldfeldPolyanskiy:20} is a recent review of many of the refs below.

\begin{itemize}
\item \cite{StrouseSchwab:17}; \href{https://github.com/djstrouse/information-bottleneck}{code}. Proposes to replace the ``soft''/stochastic cluster assignments generated by IB with ``hard''/deterministic ones through the use of $\mcal{L}_{DIB} = H(T) - \beta I(Y;T)$, where minimization still done over cluster assignments $p(t|x)$. $\mcal{L}_{DIB} - \mcal{L}_{IB} = H(T|X)$, so IB encourages stochasticity in its assignments. Claim DIB solution performs similar to IB solution in terms of IB loss, while being a significant improvement in DIB terms, while converging faster (for an Arimoto-Blahut-type algorithm.)

\item \cite{KolchinskyEtAl:18}; \href{https://openreview.net/forum?id=rke4HiAcY7}{comments}, \href{https://github.com/artemyk/ibcurve}{code}. For the case where $Y$ is a deterministic function of $X$, the MI tradeoff curve can't be explored by varying $\beta$ (because it's piecewise-linear, not concave) and for all $\beta$ find trivial solutions obtained by probabilistically ``forgetting'' a portion of $X$. Propose to fix this via modifying $\mcal{L}'_{IB} = I(X;T)^2 - \beta I(Y;T)$.\marginnote{``Units''?} Same problems arise in DIB, and are fixed with $\mcal{L}'_{DIB} = H(T)^2 - \beta I(Y;T)$.

\item \cite{RodriguezGalvezEtAl:20}; \href{https://github.com/burklight/convex-IB-Lagrangian-PyTorch}{code} clarify results of the above: $I^{2}$ can be replaced with any convex function, and this can be used to relate $\beta$ to the achieved compression rate.

\item \cite{NgampruetikornSchwab:21}; \href{https://openreview.net/forum?id=A2HvBPoSBMs}{comments}. ``Perturbation theory'' in that perturbation is done around the nonzero threshold $\beta_{c}$ below which the representation $T$ is uninformative ($I(T;Y) = I(T;X) =0$; \cite{WuEtAl:19} for more on this phenomenon). The perturbation is done around an uninformative $p_{\text{enc}}(t|x) = p(t)$. Nice but not usable for applications.

\item \cite{HuangGamal:21}; \href{https://github.com/hui811116/ib-admm}{code}. Show that convergence can be guaranteed with ADMM if the state space is augmented with the marginal $p(t)$. Legit?
\end{itemize}

\subsubsection{Deep variational IB} 
Ref: \cite{AlemiEtAl:16}; \href{https://github.com/alexalemi/vib_demo}{code}.

Makes IB implementable using a neural network for encoding/decoding. \cite{ChalkEtAl:16} does the same with kernels; claim this is more efficient.

Derivation of the variational bound: let $q(Y|T)$ be an approximation to the true decoder. Then $D_{KL}[p(Y|T), q(Y|T)] \geq 0$ implies
\begin{align*}
I(T;Y) &\geq \sum_{y,t} p(y,t) \log \frac{q(y|t)}{p(y)} = H(Y) + \sum_{y,t} p(y,t) \log q(y|t), \\
{} &\geq \sum_{x,y,t} p(x)p(y|x) p(t|x) \log q(y|t).
\end{align*}
In the last line we inserted the Markov factorization and dropped $H(Y)$ since it's independent of $T$. For a bound on the other term $I(X;T)$, we likewise need an approximation $q(t)$ to the true marginal $p(t)$. Similar considerations give \marginnote{Should be able to do better: take follow-up papers to MINE estimator or \cite{PooleOzair:19}.}
\be
I(T;X) \leq \sum_{x,t} p(x)p(t|x) \log \frac{p(t|x)}{q(t)}.
\ee
Combining these yields an upper bound on $\mcal{L}_{IB}$. 

Propose to actually evaluate this by plugging in the empirical distribution for $p(x,y)$ \marginnote{Could regularize $p(x,y)$ and do better?} and using the ``reparameterization trick'': write $t=f(x,\epsilon)$ as a deterministic function of $x$ and a Gaussian random variable $\epsilon$. Then $p(t|x)dt = p(\epsilon)d\epsilon$. Propose to do this by using a neural net to represent mean and covariance of $T$??

Note that variational formulation breaks reparameterization (copula) invariance present in real IB \cite{WieczorekEtAl:18,WieczorekRoth:20}.


\subsubsection{Variational Predictive IB} 
Ref: \cite{Alemi:20}.

Specialize above to the past-future IB case, where $X$ is the observable past of a timeseries and $Y$ is its future. Need modification because we haven't observed the future; use Markov property that $T$ and $Y$ are conditionally independent given $X$. This means $I(T;Y) = I(T;X) - I(T; X|Y)$: The conditioned MI term avoids the need to know the future: it measures the inefficiency of $T$, as measured after we know the future. Our objective is
\be
\min_{p(t|x)} I(T;X|Y) - \beta I(T;X).
\ee
Assuming the posterior $q(X|T)$ factorizes (claim this isn't necessary and can be replaced by a better approximation), this is
\be
\min_{p(t|x)} \left\langle \log\frac{p(t|x)}{q(t)} - \beta \sum_{x} \log q(x|t) \right\rangle.
\ee
Refer to \cite{AlemiFischer:18a} (\href{https://openreview.net/forum?id=HJeQToAqKQ}{comments}) for more refined approximations than used here.


\subsubsection{Conditional entropy bottleneck}
Ref: \cite{Fischer:20}, \href{https://openreview.net/forum?id=rkVOXhAqY7}{comments}; \cite{FischerAlemi:20}, \href{https://openreview.net/forum?id=SygEukHYvB}{comments}. 

Proposes to address non-informative encodings by attempting to reach the ``minimum necessary information'' point, at which $I(X;Y) = I(X;T) = I(Y;T)$; this is not always achievable. Proposes 
\begin{align*}
\min_{T} I(X; T|Y) - \gamma I(Y;T); \text{ minimized when } \\
\min_{T} -H(T|X) + H(T|Y) + \gamma H(Y|T) \text{ is.}
\end{align*}
For deterministic $X \rightarrow Y$, achieve MNI at $\gamma = 1$. Equivalent to IB at $\gamma = \beta - 1$ under Markov assumption, since then $I(X; T|Y) = I(X;T) - I(Y;T)$; not identical because we dropped $H(Y)$ in second line.

Variational implementation via learning \emph{three} functions (similar to VIB): $q_{\text{enc}}(t|x)$, such that joint $p(x,y,t) = p(x,y)q_{\text{enc}}(t|x)$; ``classifier'' $q_{\text{dec}}(y|t)$ and ``backward encoder'' $q_{\text{dec}}(t|y)$ instead of VIB's marginal $q(t)$. \marginnote{Do we need constraints to keep these three functions consistent?} Argue this gives a tighter bound than VIB (not necessarily; \cite{GeigerFischer:20}):
\be
\min_{\text{all $q$s}} \left\langle \log \frac{q_{\text{enc}}(t|x)}{q_{\text{dec}}(t|y)} - \gamma \log q_{\text{dec}}(y|t) \right\rangle_{p(x,y)q_{\text{enc}}(t|x)}.
\ee

Discusses several extensions. One is to hierarchical models $Y \leftrightarrow X = T_{0} \rightarrow T_1 \rightarrow T_2 \rightarrow \cdots$:
\be
\min_{\{T_i\}} \sum_i -H(T_{i}|T_{i-1}) + H(T_{i}|Y) + H(Y|T_{i}).
\ee
Another is the predictive IB setting. Can simply plug in $X = X_<; Y = X_\geq$ above. Can also work in the ``bidirectional'' context, where we learn two representations, $T_<$ and $T_\geq$:
\be
\min_{T_<, T_\geq} \left[ -H(T_< | X_<) + H(T_< | X_\geq ) + \gamma H(X_\geq | T_< ) \right] + [ (<t) \leftrightarrow (\geq t) ].
\ee
These are tied together by using the same encoder and backwards encoder. Introducing a fourth ``decoder'' distribution $q_{\text{enc}}(x|t)$, 
\be
\min_{\{q\}} \left\langle \log \frac{q(t_< | x_<) q(t_\geq|x_\geq)}{q(t_< | x_\geq) q(t_\geq|x_<)} - \gamma \log q(x_\geq | t_< ) q(x_< | t_\geq) \right\rangle_{p(x,y)q(t_< | x_<) q(t_\geq|x_\geq)}.
\ee

Propose to address multi-scale time series analysis by combining these objectives: each level of the hierarchy of $T_i$s would correspond to greater smoothings, conditioned on the set of $T_{i-1}$s. Reference \href{https://en.wikipedia.org/wiki/WaveNet}{WaveNet} \cite{OordEtAl:16,OordEtAl:17} as an example of a multi-scale neural architecture.


\subsection{IB applications to RL/control.}

\subsubsection{InfoBot} 
\label{sssec-infobot}
Ref: \cite{GoyalEtAl:19}.

Apply IB for regularization in RL (for increased generalization, avoiding overfitting). Specifically, want to minimize policy dependence on the goal as measured by $I(A;G|S)$. ``Goal'' $G$ seems to refer to variable but undesirable details of training data, like the location of the goal in a maze. This is equivalent to a KL regularization term where we penalize deviations of the policy from a ``default'' policy that integrates out dependence on $G$:
\be
\pi_{0} = \sum_{g} p(g) \pi(A|S,g). 
\ee
Refer to system states $S$ where we it's worth deviating from this default as ``decision states''; reward exploration by incentivizing agent to seek these out.\marginnote{Why do they think these are isolated?}

Adds an extra variable to the Markov structure of IB: $S,G \rightarrow T \rightarrow A$, but in addition $S \rightarrow A$ (formally distinguishing the roles of $S$ and $G$). Build policy from IB encoder/decoder:
\be
\pi(A | S,G) = \sum_{t} p_{\text{dec}}(A|S,t) p_{\text{enc}}(t|S,G).
\ee
Cost-to-go is $J(\pi) = \langle r \rangle_{\pi} - \beta I(A;G|S)$; we bound the MI with $I(T;G|S)$. This requires the marginal $p(T|S) = \sum_{g} p(g) p_{\text{enc}}(T|S,g)$, which is difficult (goal $G$ plays role of the ``future''; we probably have poor knowledge about the out-of-sample $p(G)$ the agent will encounter.) Replace it with a variational approximation $q(T|S)$ to get the lower bound used in practice:
\be
J(\pi) \geq \mtilde{J}(\pi) = \left\langle r - \beta D_{KL}[p_{\text{enc}}(T|S,G) | q(T|S)] \right\rangle_{\pi},
\ee
with parameter update rule (under the ``Reinforce'' algorithm; Monte Carlo policy gradient)
\be
\nabla_{\theta} \left.\mtilde{J}(\pi)\right|_{t} = \left( \sum_{t'=t}^{T} \gamma^{t'-t}\mtilde{r}_{t'} \right) \log\pi(a_t | s_t, g_t) - \beta \nabla_{\theta} D_{KL}[p_{\text{enc}}(T|s_t, g_t) | q(T|s_t)],
\ee
where the modified reward
\be
\widetilde{r}_{t} = r_{t} + \beta D_{KL}[p_{\text{enc}}(T|s_t, g_t) | q(T|s_t)].
\ee

Again, still seeking to maximize reward, rather than an info-theoretic quantity, which distinguishes this from IB. Correspondence with variational IB is recovered if we were to replace $\langle r \rangle$ with $I(A^\ast; A|S)$, where $A^\ast$ is the true optimal action --- this gives the VIB objective between $G$ and $A^\ast$, conditioned on $S$. 


\subsubsection{Variational Bandwidth bottleneck} 
Ref: \cite{GoyalEtAl:19a}.

One problem with variational IB is that encoder needs access to full input training, so itself can be overfitted, in the sense that it might not compress new inputs. Fix by defining two classes of input: ``standard'' $S$ and ``privileged'' $G$ (assumed independent here). These deliberately map onto the state/goal representations in InfoBot. We want to avoid using $G$, either because we want to generalize with respect to it, or because it's intrinsically expensive to obtain/calculate\marginnote{For example, it could be model output...}. Now we minimize conditional MI between $T$ and $G$ given $S$; the algorithm makes decisions on whether to invoke $G$ before looking at it (InfoBot always accesses $G$.)

With variables and Markov dependencies as in InfoBot, the bound used is
\be
I(T;G|S) \leq \sum_{s,g} p(s)p(g) D_{KL}[p_{\text{enc}}(T|s,g) | q(T)].
\ee

How do we decide when to use $G$? Refer to a ``budget'' (channel capacity $d_c$, taken between 0 and 1). Could just stochastically choose between a deterministic encoder $f(S,G)$, and the ``prior'' $q(T)$. This binary choice is non-differentiable, though. Instead define a function $d_c = B(S)$, to be parameterized by a NN. 
\be
D_{KL}[p_{\text{enc}}(T|s,g) | q(T)] = -d_c \log d_c + (1-d_c)\left\{ \log p(f(s,g)) - \log [d_c p(f(s,g)) + (1-d_c)] \right\}.
\ee

In here as in InfoBot, assume $q(T)$ is a unit Gaussian. Why? Not sensitive to details?

Need to see an implementation for this to make sense.



\subsubsection{Predictive info soft actor-critic}
Ref: \cite{LeeEtAl:20}; \href{https://github.com/google-research/pisac}{code}.


[...]


\subsubsection{Robust predictable control}
Ref: \cite{EysenbachEtAl:21a}; \href{https://ben-eysenbach.github.io/rpc/}{code}.

[...]


\section{Variational autoencoders}

``GANs bypass any inference of latent variables, and auto-regressive models abstain from using latent variables. VAEs jointly learn an inference model and a generative model, allowing them to infer latent variables from observed data.''



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Inital write-up of handwritten notes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Keyword: \emph{approximate Bayesian computation}, when observations/likelihood evaluations are expensive. \emph{Active learning}/\emph{experimental design}, in that we want to plan likelihood evaluations to maximize information gained from each.

\section{First batch}

\subsection{Huan RL for experimental design}

``Passive POMDP'': Want actions to only change knowledge, not the observable.

World dynamics $p(W_{t}| W_{t-1})$, $t$-indep. Agent \emph{memory} $q_{t}(M_{t}|M_{t-1}, O_{t})$. Noisy observations $\sigma(O_{t}| W_{t})$. Cost to be minimized: $\langle \tfrac{1}{N} \sum_{t} d(W_{t}, M_{t}) \rangle$. 

Unbounded agent: let $M_{t}$ be belief state (about $W$, given \emph{all} $\{ O_{t} \}$, i.e. $M_{t} = B_{t}( W_{t} | \{ O_{t} \})$. Bayes update is
\be
B_{t}( W_{t} | \{ O_{t} \}) \propto \sum_{W_{t-1}} B_{t-1}( W_{t-1} | \{ O_{t-1} \})p(W_{t}| W_{t-1})\sigma(O_{t}| W_{t}).
\ee
(so Huan turns off dynamics for $W_{t} = \theta$; $\sigma(\cdot)$ is $y = G(\theta) + \eta$).

Versus Huan's terms: experiment index $k = 1 \ldots N$, belief state $X^{B}_{k}$, physical state $X_{k}$. Design $d_{k}$ is parameters for $k$th experiment, or policy $\pi_{k}(X_{k}) = d_{k}$. Observations $y_{k}$ taken to be $G_{k}( \theta, d_{k}) + \eta$, with static parameters $\theta$. Let $z_{k} = (y_{k}, d_{k})$, and $Z_{k}$ be the full history of $z$s up to experiment $k$.

Can write Bellman for this. Reward/cost $g_{k}(x,y,d)$ with terminal $g_{N}(x_{N})$. Dynamical update $X_{k+1} = F_{k}(x,y,d)$.

Bayes for $\theta$:
\be
p(\theta | Z_{k})  = \frac{p(y_{k} | \theta, d_{k}, Z_{k-1})}{p(y_{k}| d_{k}, Z_{k-1})}p(\theta | Z_{k-1}).
\ee

Set terminal reward to info we've gained from all experiments:
\be
g_{N} = D_{KL}[ p(\theta|Z_{N}) | p(\theta | \emptyset) ],
\ee
claim this means Bellman is non-convex? Each $g_{k}$ is the cost (in terms of resources) to run each experiment. 

Then Bellman
\be
J_{k}(x_{k}) = \max_{d_{k}} \langle g_{k} + J_{k+1}(F) \rangle
\ee
\be
\pi_{k}(x_{k}) = \arg \max_{d_{k}} Q_{k}(x,d).
\ee
(Cost-to-go $J_k$ is the ``critic''; $\pi$ is the ``actor.'') ``Model free'' since can go $Q \rightarrow \pi$ without explicit knowledge of $F$.

\begin{notebox}
Remarks:
\begin{enumerate}
\item Bring in Tishby stuff for dimensionality, instead of directly discretizing pdfs.
\item How's $D_{KL}$ deal with singular models?
\item In 2021 paper, get exploration via noise (sec. 3.2.4). Want to do this via entropy-like term in $g_{k}$.
\item MINE-type estimators for $D_{KL}$.
\item 2021 paper uses vanilla actor-critic; bring in soft actor-critic.
\item Setting is for experimental design. What about optimization, i.e. ``find $\theta$ in best agreement with summary statistics''? Just tack on summary statistic and comparison into $G$? Could plug into CES: $\{ y_{k} \}$ are the summary statistics, $\{ d_{k} \}$ are the model parameters for each run.
\end{enumerate}

\end{notebox}

\subsection{CES again}


\begin{notebox}
\begin{itemize}
{}
\item Issue with CES is that the stages don't ``know about'' each other, but do target max posterior density. Minimize cost of emulator evaluations for MCMC --- e.g. with random features. Dimensional reduction 1: only stuff for \emph{functions} of $y$: suffice to do $p(y|\theta)$? Or could compress $\theta \rightarrow y$ itself.
\item ``C'' then separate ``S'' feels redundant. 

\item Emulator serves a necessary function of smoothing data (purely from $\eta$ in this model, right? Spectrum?) but done ad hoc. Does enKF break on multimodal distributions?

\item Calibrate $\rightarrow$ enKF $\rightarrow$ PI control a la Kappen? Or ``smoothing'' = Sample? Want RL agent to plan model evaluations; smoothing then done offline.

\item Can we do anything clever with FDT or differentiation with respect to $\theta$s to estimate $\cov(\widehat{y}, \theta)$? Or $\theta$ dependence given GP kernel?
\end{itemize}
\end{notebox}

\section{Second batch}

\section{Third batch}

\subsection{Normalizing flows for inverse inference}
\subsection{Liu-Wang '16}
\subsection{Marzouk ATM algorithm}
\subsection{SNPLA}
\subsection{SNPE-C}
Ref: Greenberg '19

Problem: after first $\{x, \theta\}$ samples, want to update our proposal, \emph{but} if we don't use prior, we don't learn the real posterior.

\subsection{Contrastive learning}
Ref: Durkan '20; nothing new on SNPE-C per se.

\subsection{SNVI}
Better likelihood?

\subsection{Active learning for SNPE-C/SNVI}

\subsection{Blau '22}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Applications}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Data assimilation}


\section{Portfolio theory}

\subsection{Motivation: betting strategies.}

\subsubsection{Kelly criterion} \cite{CoverThomas:91} chapter 6; Kelly 1956.

Let $I$ be a discrete random variable representing the winner of a horse race. Horse $i$ has probability $p_{i}$ to win; assume a bookmaker offers odds $1/q_{i}$ on this horse. For now assume the odds are ``fair'' in that $\sum_{i} q_{i} = 1$; more realistic is ``subfair'' odds $\sum_{i} q_{i} > 1$, where the house always takes a cut. A gambler bets a fraction $f_{i}$ of their bankroll on horse $i$; $\sum f_{i} = 1$. The relative payoff is $f_{i}/q_{i}$ if $i$ wins, otherwise $0$. 

For a single race, we might want to put everything on the bet with the largest expected payoff $\langle 1/q_{i} \rangle_{p}$, although this carries a large chance of being wiped out completely. To quantify risk, we instead consider the setting of maximizing terminal wealth after a series of repeated races. Treating the wealth $V_{n}$ after $n$ races as a stochastic variable, 
\be
V_n (I_{1}, I_{2}, \ldots, I_{n}) = V_{0} \prod_{t=1}^{n} \frac{f_{i,t}}{q_{i,t}};
\ee
(assume probabilities and odds remain constant). We've assumed race outcomes $\{I_{1}, I_{2}, \ldots, I_{n} \}$ are iid $\sim p(x)$. The product structure, which arises simply due to the fact that we can reinvest our previous winnings, means that $\{ \log f_{i,t}/q_{i,t} \}_{t}$ are also statistically independent. Then invoke the weak law of large numbers [i.e., for large $n$ outcome $i$ will happen $np_{i}$ times] to deduce that \marginnote{Understand terms of this statement better: ``convergence in probability.''}
\be
\frac{1}{n} \log V_{n} \underset{p}{\longrightarrow} \left\langle \log \frac{f_{i}}{q_{i}} \right\rangle_{p} = \left\langle \log \frac{f_{i}}{p_{i}} \frac{p_{i}}{q_{i}} \right\rangle_{p} = D_{KL}(p|q) - D_{KL}(p|f), 
\ee
expressed in terms of KL divergences. Terminal wealth grows exponentially with $n$ as $\langle \log f_{i}/q_{i} \rangle_{p}$, and this latter quantity is what we want to maximize. Include a lagrange multiplier $\lambda$ enforcing $\sum_{i} f_{i}$ = 1, then 
\be
\arg \max_{f} \left[ \langle \log f_{i}/q_{i} \rangle_{p} + \lambda \sum_{i} f_{i} \right] \Rightarrow f^{\ast}_{i} = - \frac{p_{i}}{\lambda^{\ast}} = p_{i},
\ee
with the optimal growth rate
\be
W^{\ast}[I] = \left\langle \log \frac{f^{\ast}_{i}}{q_{i}} \right\rangle_{p} = - \langle \log q_{i} \rangle_{p} - H[I].
\ee
This is Kelly's result (``proportional gambling.'') Returning to the KL divergence interpretation, we can only make money (again, in the large-$n$ sense) if our estimate of the true distribution $p$, as expressed through our bets $f$, is closer in the KL sense than the bookmaker's estimate $q$.


\subsubsection{Side information in the horse race.} Same assumptions as above, but we want to condition our strategy on an arbitrary stochastic variable $Y$. 

Formally, need only replace $p_{i} \rightarrow p(i, y)$ and $f_{i} \rightarrow f(i|y)$ in the derivation above; now have conditioned growth rate
\be
W^{\ast}[I|Y] = \sum_{i,y} p(i,y) \log \frac{f^{\ast}(i|y)}{q_{i}} = - \langle \log q_{i} \rangle_{p} - H[I|Y].
\ee
This is still optimized by $f^{\ast}(i|y) = p(i|y) = p(i,y)/p(y)$. 

We see that $W^{\ast}[I|Y] - W^{\ast}[I] = I(I;Y)$: the increase in growth rate provided by knowing $y$ is equal to the mutual information $Y$ provides about the race outcome $I$. In a more realistic portfolio scenario, this is only an upper bound, obtained when the market takes the horse racing form. 


\subsubsection{Optimal gambling is optimal coding/compression} Both concern themselves with estimation of the distribution $p$ underlying an iid process $\{I_{1}, I_{2}, \ldots, I_{n} \}$. Made explicit in \cite{CoverThomas:91} 6.5.

[...]


\subsubsection{Observations on Kelly}
 
\begin{itemize}
\item The above criteria don't tell us how to find an edge, only how to best exploit it once it's identified. If the bookmaker offers perfect odds $q(x) = p(x)$, the optimal solution is not to bet at all [after first extending the scenario to include this option; optimal $f^{\ast}$ in this case now depends on the offered odds $1/q(x)$].

\item We assumed stationarity across different races, duh.

\item It may take an infeasibly long time to reach the asymptotic growth regime. You might not be able to find enough opportunities with edge to make it into this regime.

\item \emph{Opportunity costs}: the Kelly fraction for an asset can only be calculated with reference to the entire universe of opportunities. ``Common error,'' since it leads to overestimating the fraction, and Kelly is generally the most concentrated it makes sense to be [Thorp 08].
\end{itemize}



\subsection{Optimal portfolios.}

\subsubsection{Kelly criterion in the portfolio context}

The horse race may be viewed as a specialized case of the general portfolio allocation problem (in which ``returns'' are only accumulated for one period with a winner-take-all structure.)

[...]

The above considerations suggest maximizing $\langle \log r \rangle$. This is at odds with classical portfolio theory due to Markowitz, which regards the allocation question as a tradeoff between risk and expected return. \marginnote{BUT log-optimal reduces to mean-variance in a limit [which?]} 

Properties of the log-optimal portfolio [in MacLean et al 11]:
\begin{itemize}
\item Log utility maximizes asymptotic growth; it's \emph{myopic}/greedy in the sense that maximization period-by-period (independent of history) yields the global maximum [Kelly 56]. 
\item The ratio of a log-optimal portfolio to any ``essentially different'' strategy (must differ from it an extensive fraction of the time, but this is not sufficient; this is subtle. See [Thorp 08].) grows asymptotically without bound. The log-optimal portfolio minimizes the expected time to reach a value threshold. [Breiman 61].
\item For arbitrary return distribution, log-optimality maximizes asymptotic growth rate [Algolet and Cover 88].
\item The log-optimal portfolio is always on the geometric mean-variance frontier (geometric rather than arithmetic means are appropriate for the multi-period setting). Arithmetic MV-efficient porfolios are generally not geometrically efficient, and vice versa [Thorp 71].
\item Log utility is myopic for general return distributions [Hakansson 71].
\end{itemize}


\subsubsection{Growth-optimal portfolios} \cite{CoverThomas:91} ch. 16; \cite{Cesa-BianchiLugosi:06} ch. 10.


In an iterated context, the principle underlying Cover's universal portfolio is \emph{volatility harvesting}, an essentially mean-reverting strategy (if only because volatility, by itself, has no drift by definition). As a minimal example, consider two assets, one of which remains constant in value and the other of which oscillates at a fixed frequency. If our portfolio is \emph{constantly rebalanced} to maintain a constant ratio of value between the two assets, we sell the oscillating asset at its highs and buy it at its lows.

This is independent of a choice of the stochastic process generating the returns. 

This is infeasible in practice, because ``volatility'' is a high-frequency phenomenon, so this would require high volatility \marginnote{What about multiply-leveraged ETFs?} and infeasibly frequent rebalancing. It would fall short in 

[...]

[ what's the relationship between Cover and Kelly?]

Claim for the Cover portfolio is that asymptotic performance is equal to the best constant [wealth fraction] rebalanced portfolio, chosen in hindsight (i.e., with access to future price series.)



\subsection{Control/RL for the trading problem.} 

Trading as the full problem, including execution/timing, as opposed to portfolio optimization.

As described above, impact of Kelly is just to use $\langle \log r \rangle$ as the objective in the control problem. Non-linear reward means Bellman recursion is non-linear; rules out some algorithms.

\subsubsection{Framing the problem.} 

Ideally we'd like a hierarchy of approximations, which could be expressed perturbatively in respective algorithms.
\begin{enumerate}
\item Roughest to assume zero impact: this is would be MDP where state is \{prices, holdings\}, and control inputs (policy actions; trades) only affect the latter.
\item Could then add a rough slippage/impact model to let trades affect prices directly.
\item Next step would be to add estimation of the parameters of the stochastic process underlying prices to control.
\item Ditto for parameters of slippage model.

\end{enumerate}

Remark that vanilla RL addresses the case where the reward function is unknown and can only be determined through active exploration. Neither of these assumptions apply to trading in the regimes we'd want to be active in: 
\begin{itemize}
\item Prices are transparent and we can mark-to-market at any time. This breaks down when assets are hard to value (through not only illiquidity but non-standardization: say real estate or residential MBSes).
\item Again due to price transparency, we can backtesting a hypothetical strategy straightforwardly. This breaks down in situations of high slippage/impact (illiquidity), closer to a multi-armed bandit problem in which we can only use the information of market impact from our previous trades.
\end{itemize}

See \cite{BoydEtAl:17} for implementation of the MDP problem.

Refer to sec.~\boldref{sec-control-vs-ml}. 
\begin{itemize}
\item POMDP $\rightarrow$ belief MDP. Kelly (or rather, its Bayesian extension) is a deterministic mapping from beliefs to bet sizes (actions).
\item 

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% References

\clearpage
\newgeometry{total={5.75in, 9in}} % https://stackoverflow.com/questions/1670463
% \section{References}

% reduce spacing between bibliography items
\let\oldthebibliography=\thebibliography
\let\endoldthebibliography=\endthebibliography
\renewenvironment{thebibliography}[1]{%
    \begin{oldthebibliography}{#1}%
    \setlength{\parskip}{3pt plus 2pt minus 1pt}%
    \setlength{\itemsep}{3pt}%
}%
{%
    \end{oldthebibliography}%
}

% put biblio in small text, 2-column environment
\begin{adjmulticols*}{2}{-2cm}{-1.5cm}[\section*{References}] % expand margins

\noindent
%\begin{adjmulticols}{2}{-2cm}{-2cm} % expand margins
\footnotesize %\small %
\begin{flushleft}
\bibliographystyle{alphaurl}
\bibliography{info-optim-control,info-optim-control-2}
%\printbibliography
\end{flushleft}
\normalsize

\end{adjmulticols*}

\end{document}
 